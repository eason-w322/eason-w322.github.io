<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Project 4: NeRF — Part 0</title>
<link rel="stylesheet" href="style.css" />
</head>

<body>

<header>
  <h1>Project 4: Neural Radiance Fields (NeRF)</h1>
  <h2>Eason Wei — CS180 / Fall 2025</h2>
</header>

<!-- ===== Introduction ===== -->
<section id="intro" class="card">
  <h2>Introduction</h2>
  <p>
    In this project, I implement a full pipeline for reconstructing a 3D scene using 
    <b>Neural Radiance Fields (NeRF)</b>. I begin by <b>calibrating my camera</b> with ArUco markers 
    to recover intrinsics and camera poses. Then, I train a <b>2D neural field</b> to understand how 
    positional encoding and MLPs represent continuous signals. Finally, I build and train a complete 
    <b>3D NeRF</b>, generating rays, sampling points, performing volume rendering, and producing 
    novel-view images and GIFs of the reconstructed object.
  </p>
</section>

<!-- ===== Part 0 ===== -->
<section id="part0">
  <h2>Part 0 — Camera Calibration & 3D Object Scanning</h2>

  <!-- ===== 0.1 Camera Calibration ===== -->
  <h3>0.1 Camera Calibration</h3>
  <p>
    In this section, I printed a 6-tag ArUco grid and captured 50 images of it. Since the originals were 
    in <code>.heic</code> format and very large, I converted them to <code>.jpg</code> using a Python script. 
    I measured each printed tag to be 0.57 units wide and used this measurement to define the 3D coordinates 
    of all tag corners. Using <code>cv2.aruco.detectMarkers</code>, I extracted the 2D corner locations and 
    applied a mask to correctly index all six tags. Finally, I computed the intrinsic matrix <b>K</b> and 
    distortion coefficients using <code>cv2.calibrateCamera</code>.
  </p>

  <!-- ===== 0.2 Object Capture ===== -->
  <h3>0.2 Object Capture</h3>
  <p>
  For my object, I selected a Labubu figure and placed it beside a single printed ArUco tag on a flat tabletop. 
  I captured another 50 images from different viewpoints while keeping the camera settings and distance 
  consistent (approximately 10–20 cm away so the object fills about half of the frame). To ensure good NeRF 
  training quality, I avoided exposure changes, minimized motion blur, and varied the viewing angle both 
  horizontally and vertically while maintaining a uniform radius around the object.
  </p>

  <!-- ===== 0.3 Pose Estimation ===== -->
  <h3>0.3 Pose Estimation (PnP)</h3>

<p>
  To estimate the camera pose for each Labubu image, I detect the ArUco tag and compute a full 
  <b>camera-to-world</b> transformation matrix. After resizing the images and scaling the intrinsic 
  matrix <code>K</code>, I use <code>cv2.solvePnP</code> to recover the rotation and translation that map 
  the tag's known 3D geometry onto the observed 2D corner locations.
</p>

<p>
  Given 3D object points \( X_i \in \mathbb{R}^3 \) and their detected pixel coordinates 
  \( x_i \in \mathbb{R}^2 \), <code>solvePnP</code> estimates the camera pose by solving
</p>

<p class="math-block">
\[
x_i \sim K \, [R \mid t] \, 
\begin{bmatrix} X_i \\ 1 \end{bmatrix}.
\]
</p>

<p>
  The function returns an axis–angle rotation vector \( \mathbf{r} \) and a translation vector 
  \( \mathbf{t} \). I convert the rotation vector to a rotation matrix using
</p>

<p class="math-block">
\[
R = \mathrm{Rodrigues}(\mathbf{r}).
\]
</p>

<p>
  From this, I first construct the <b>world-to-camera</b> matrix:
</p>

<p class="math-block">
\[
\mathrm{w2c} = 
\begin{bmatrix}
R & \mathbf{t} \\
\mathbf{0}^\mathsf{T} & 1
\end{bmatrix}.
\]
</p>

<p>
  The <b>camera-to-world</b> matrix is its inverse, which has a closed-form expression:
</p>

<p class="math-block">
\[
\mathrm{c2w} = 
\begin{bmatrix}
R^\top & -R^\top \mathbf{t} \\
\mathbf{0}^\mathsf{T} & 1
\end{bmatrix}.
\]
</p>

<p>
  Since NeRF uses a different coordinate convention from OpenCV, I transform all poses with 
  a diagonal flip matrix
</p>

<p class="math-block">
\[
D = \mathrm{diag}(1,\,-1,\,-1),
\qquad
R_{\text{nerf}} = D R^\top, 
\qquad
\mathbf{t}_{\text{nerf}} = D(-R^\top \mathbf{t}).
\]
</p>

<p>
  Each final pose is a full \(4 \times 4\) camera-to-world matrix compatible with volume rendering
  in NeRF. All valid poses and resized images are saved into an 
  <code>.npz</code> dataset and visualized using <b>Viser</b>.
</p>
  <p>Insert Viser camera frustum plots here.</p>
  <div class="img-container">
    <img src="images/viser_cam_plot1.png" alt="Viser Camera Plot 1" />
    <img src="images/viser_cam_plot2.png" alt="Viser Camera Plot 2" />
  </div>

  <!-- ===== 0.4 Dataset Packaging ===== -->
  <h3>0.4 Dataset Packaging</h3>
  <p>Insert explanation + final dataset statistics.</p>

</section>

</body>
</html>
