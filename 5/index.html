<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

<title>Project 4: NeRF — Part 0</title>
<link rel="stylesheet" href="style.css" />
</head>

<body>

<!-- ===== HEADER ===== -->
<header class="site-header">
  <h1>Project 4: Neural Radiance Fields (NeRF)</h1>
  <p class="subtitle">Eason Wei — CS180 / Fall 2025</p>
</header>

<!-- ===== MAIN CONTENT ===== -->
<main class="container">

<!-- ===== Introduction ===== -->
<section id="intro" class="card">
  <h2>Introduction</h2>
  <p>
    In this project, I implement a full pipeline for reconstructing a 3D scene using
    <b>Neural Radiance Fields (NeRF)</b>. The workflow begins with <b>camera calibration</b> using
    ArUco markers to recover intrinsics and poses. I then train a <b>2D neural field</b> to study
    positional encoding and MLP representations. Finally, I build a full <b>3D NeRF</b> that
    generates rays, samples points, performs volume rendering, and produces novel-view images
    and GIFs of my captured object.
  </p>
</section>

<!-- ===== Part 0 ===== -->
<section id="part0" class="card">
  <h2>Part 0 — Camera Calibration & 3D Object Scanning</h2>

  <!-- 0.1 -->
  <h3>0.1 Camera Calibration</h3>
  <p>
    I printed a 6-tag ArUco grid and captured 50 images of it. Because the raw photos were
    large <code>.heic</code> files, I converted them to <code>.jpg</code>. Each printed tag was
    measured to be 0.57 units wide, which I used to construct the 3D corner coordinates.
    Using <code>cv2.aruco.detectMarkers</code>, I extracted the 2D corners and applied a mask to
    index the 6 tags consistently. Finally, I computed the intrinsic matrix <b>K</b> and
    distortion coefficients with <code>cv2.calibrateCamera</code>.
  </p>

  <!-- 0.2 -->
  <h3>0.2 Object Capture</h3>
  <p>
    For my object, I selected a Labubu figure and placed it beside a single printed ArUco tag
    on a tabletop. I captured around 50 images from different viewpoints while maintaining
    consistent distance (10–20 cm) and stable camera settings. To ensure good NeRF training,
    I avoided exposure changes, minimized motion blur, and varied the viewing angle both
    horizontally and vertically while keeping a uniform radius around the object.
  </p>

  <!-- 0.3 -->
  <h3>0.3 Pose Estimation (PnP)</h3>

  <p>
    For each Labubu image, I detect the ArUco tag and estimate the camera pose by solving the
    Perspective-n-Point (PnP) problem. Given 3D tag coordinates and their detected 2D projections,
    <code>cv2.solvePnP</code> estimates rotation and translation satisfying:
  </p>

  <p class="math-block">
  \[
  x_i \sim K \, [R \mid t] \,
  \begin{bmatrix} X_i \\ 1 \end{bmatrix}.
  \]
  </p>

  <p>
    The rotation vector \( \mathbf{r} \) is converted to a matrix using Rodrigues:
  </p>

  <p class="math-block">
  \[
  R = \mathrm{Rodrigues}(\mathbf{r}).
  \]
  </p>

  <p>
    I construct the world-to-camera matrix:
  </p>

  <p class="math-block">
  \[
  \mathrm{w2c} =
  \begin{bmatrix}
  R & \mathbf{t} \\
  \mathbf{0}^\mathsf{T} & 1
  \end{bmatrix}.
  \]
  </p>

  <p>
    Its inverse gives the camera-to-world matrix:
  </p>

  <p class="math-block">
  \[
  \mathrm{c2w} =
  \begin{bmatrix}
  R^\top & -R^\top \mathbf{t} \\
  \mathbf{0}^\mathsf{T} & 1
  \end{bmatrix}.
  \]
  </p>

  <p>
    Since NeRF uses a different axis convention than OpenCV, I apply the flip matrix:
  </p>

  <p class="math-block">
  \[
  D = \mathrm{diag}(1, -1, -1),
  \qquad
  R_{\text{nerf}} = D R^\top,
  \qquad
  \mathbf{t}_{\text{nerf}} = D(-R^\top \mathbf{t}).
  \]
  </p>

  <p>
    Each final pose is a full \(4 \times 4\) camera-to-world matrix used for NeRF rendering. All
    valid poses and resized images are saved into an <code>.npz</code> file and visualized using Viser.
  </p>

  <div class="img-container">
    <img src="images/viser_cam_plot1.png" alt="Viser Camera Plot 1" />
    <img src="images/viser_cam_plot2.png" alt="Viser Camera Plot 2" />
  </div>

  <!-- 0.4 -->
  <h3>0.4 Dataset Packaging</h3>
  <p>
    Insert explanation + dataset statistics here.
  </p>

</section>

</main>

<!-- ===== MathJax ===== -->
<script>
  window.MathJax = {
    tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
    svg: {fontCache: 'global'}
  };
</script>
<script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</body>
</html>
