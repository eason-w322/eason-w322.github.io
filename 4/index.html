<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 • Project 2 — Part 1</title>
  <link rel="stylesheet" href="style.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
  <header class="page-header">
    <h1>CS180 — Project 2</h1>
    <h2>Part 1: Fun with Filters</h2>
  </header>

  <main class="container">

    <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2 class="section-title">Part 1</h2>

      <!-- =========== Part 1.1 =========== -->
      <article id="part1-1">
        <h3 class="section-title">1.1 Convolutions from Scratch</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Grayscale Selfie &amp; Derivatives)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_gray.png" />
            <figcaption>Original (Grayscale)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_box_scipy.png" alt="Selfie after convolution (box filter)" />
            <figcaption>After Convolution (Box Filter)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Ix.png" alt="Selfie Lx" />
            <figcaption>Finite Difference: L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Iy.png" alt="Selfie Ly" />
            <figcaption>Finite Difference: L<sub>y</sub></figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          The four nested for-loop implementation of convolution is extremely slow. 
          Efficiency improves significantly when switching to the two for-loop version, 
          where I pad the image with zeros using <code>np.pad</code> based on the kernel size. 
          My self-implemented convolution functions produce the exact same output as the built-in 
          <code>scipy.signal.convolve2d</code>. Compared to the original grayscale selfie, the box-filtered 
          result appears blurred. Additionally, convolving the image with D<sub>x</sub> and D<sub>y</sub> 
          produces the corresponding derivative images shown above.
        </p>

        <details class="code-toggle">
          <summary>Show / Hide my convolution code</summary>

          <div class="code-columns">
            <figure class="code-card">
              <figcaption class="code-title">Four-Loop Convolution (no padding)</figcaption>
<pre class="code-block"><code>def conv2d_four_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    ch, cw = kh // 2, kw // 2
    k   = _flip_kernel(kernel).astype(np.float32, copy=False)
    img = img.astype(np.float32, copy=False)
    out = np.zeros((H, W), dtype=np.float32)

    for i in range(H):
        for j in range(W):
            s = 0.0
            for u in range(-ch, ch + 1):
                for v in range(-cw, cw + 1):
                    ii = i + u
                    jj = j + v
                    ku = ch + u
                    kv = cw + v
                    if 0 &lt;= ii &lt; H and 0 &lt;= jj &lt; W:
                        s += img[ii, jj] * k[ku, kv]
            out[i, j] = s
    return out</code></pre>
            </figure>

            <figure class="code-card">
              <figcaption class="code-title">Two-Loop Convolution (zero padding)</figcaption>
<pre class="code-block"><code>def conv2d_two_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    k = _flip_kernel(kernel)
    ch, cw = kh // 2, kw // 2
    out = np.zeros((H, W), dtype=np.float32)
    padded = np.pad(img, ((ch, ch), (cw, cw)), mode='constant', constant_values=pad_value)
    for i in range(H):
        for j in range(W):
            window = padded[i:i+kh, j:j+kw]
            out[i, j] = np.sum(window * k, dtype=np.float32)
    return out</code></pre>
            </figure>
          </div>
        </details>
      </article>

      <!-- =========== Part 1.2 =========== -->
      <article id="part1-2">
        <h3 class="section-title">1.2 Edge Detection with Finite Differences</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Edge Maps &amp; Gradient)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Ix.png" alt="Lx Gradient" />
            <figcaption>Gradient L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Iy.png" alt="Ly Gradient" />
            <figcaption>Gradient L<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/gradient.png" alt="Gradient Magnitude" />
            <figcaption>Gradient Magnitude</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/binarize_edges.png" alt="Binarized Edges" />
            <figcaption>Binarized Edge Map</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Using the finite difference operators D<sub>x</sub> and D<sub>y</sub>, I computed the partial
          derivatives of the image in both the horizontal and vertical directions. Combining these
          results gives the gradient magnitude map, which highlights edges regardless of orientation.
          By applying a threshold of 0.32~0.34 the gradient magnitude is converted into a binarized edge map, which successfully eliminating most of the background noise
          while outlining the major boundaries in the image.
        </p>
      </article>

      <!-- =========== Part 1.3 =========== -->
      <article id="part1-3">
        <h3 class="section-title">1.3 Derivative of Gaussian (DoG) Filter</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Gaussian Smoothing &amp; DoG)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/edges_blur.png" alt="Gaussian Blur edge map" />
            <figcaption>Gaussian Blur edge map</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/edges_DoG.png" alt="DoG Gaussian edge map" />
            <figcaption>DoG Gaussian edge map<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/gradient_blur.png" alt="Gaussian Blur gradient" />
            <figcaption>Gaussian Blur gradient<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/gradient_DoG.png" alt="DoG Gaussian Blur gradient" />
            <figcaption>DoG Gaussian Blur gradient</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Applying finite difference operators directly on the original image (as in Part 1.2) often produces noisy edge maps.
          By first smoothing the image with a Gaussian filter (as in Part 1.3), noise is reduced and edges appear sharper and more coherent. 
          The Derivative of Gaussian (DoG) filters combine smoothing and differentiation into a single operation, producing results equivalent to 
          applying Gaussian blur followed by convolution with D<sub>x</sub> or D<sub>y</sub>.
          
          Interestingly, while both approaches yield visually identical binarized edge maps at a threshold of 0.2, their gradient magnitudes differ 
          slightly. The DoG results appear somewhat brighter compared to the Gaussian-blur-then-differentiate approach. This discrepancy is likely due to 
          subtle differences in numerical precision and the order of operations: in the DoG method, the derivative is embedded in the Gaussian kernel itself, 
          whereas in the two-step method, smoothing and differentiation are performed sequentially
        </p>
      </article>

<!-- =========== Part 2.1 =========== -->
<article id="part2-1">
  <h3 class="section-title">2.1 Image Sharpening</h3>

  <!-- Top Row -->
  <h4 class="section-title">Taj Mahal Example</h4>
  <div class="image-strip" role="list">
    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/0_original.png" alt="Original Taj" />
      <figcaption>Original</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/1_blurred.png" alt="Blurred Taj" />
      <figcaption>Blurred</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/2_highfreq_vis.png" alt="High-Freq Taj" />
      <figcaption>High-Frequency</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/3_sharp_two_step.png" alt="Sharpened Taj" />
      <figcaption>Sharpened</figcaption>
    </figure>
  </div>

  <p class="explainer-text">
  In this part, I first applied a Gaussian blur to the original Taj image. 
  Subtracting the blurred result from the original yields the high-frequency component. 
  This high-frequency detail is then scaled by a factor <em>α</em> and added back to the original image. 
  The parameter <em>α</em> controls how much detail is reintroduced: higher values produce a crisper 
  but potentially noisier result. The sharpened Taj image demonstrates enhanced edges and fine details.
</p>

<div class="equation-box">
  <p><strong>Sharpening Formula:</strong></p>
  <p>
    $$ I_{\text{sharp}} \;=\; I \;+\; \alpha \left( I - (G_{\sigma} * I) \right) $$
  </p>
</div>
  <!-- Bottom Row -->
  <h4 class="section-title">Pyramid Example</h4>
  <div class="image-strip" role="list">
    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/0_original.png" alt="Original Pyramid" />
      <figcaption>Original</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/5_demo_blurred.png" alt="Blurred Pyramid" />
      <figcaption>Blurred</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/2_highfreq_vis.png" alt="High-Freq Pyramid" />
      <figcaption>High-Frequency</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/6_demo_resharpened.png" alt="Resharpened Pyramid" />
      <figcaption>Resharpened</figcaption>
    </figure>
  </div>

  <p class="explainer-text">
    In the pyramid example, the resharpened image cannot be fully restored to its original quality because the high-frequency 
    details were already lost during the initial Gaussian blur. When applying the unsharp mask, we blur the image again to compute the 
    high-frequency component. However, since this is done on an already blurred image, many fine details are irretrievably lost. 
    As a result, the resharpening process only adds back a limited portion of the high-frequency content, making the 
    final image appear sharper than the blurred version but still noticeably less detailed compared to the original.
  </p>
</article>
      <!-- =========== Part 2.2 =========== -->
      <article id="part2-2">
        <h3 class="section-title">2.2 Hybrid Images</h3>

        <h4 class="section-title">Hybrid Example 1: Full Process</h4>

        <!-- Originals -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/data/dog.jpg" alt="Original High Source" />
            <figcaption>Dog (High-Frequency Source)</figcaption>
          </figure>
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/data/cat.jpg" alt="Original Low Source" />
            <figcaption>Cat (Low-Frequency Source)</figcaption>
          </figure>
        </div>

        <!-- FFT of Originals -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_orig_high.png" alt="FFT Original High" />
            <figcaption>FFT Dog</figcaption>
          </figure>
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_orig_low.png" alt="FFT Original Low" />
            <figcaption>FFT Cat</figcaption>
          </figure>
        </div>

        <!-- Filtered -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_highpass_vis.png" alt="High-Pass Filtered" />
            <figcaption>High-Pass Dog (from Original 1)</figcaption>
          </figure>
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_lowpass.png" alt="Low-Pass Filtered" />
            <figcaption>Low-Pass Cat (from Original 2)</figcaption>
          </figure>
        </div>

        <!-- FFTs -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_fft_lowpass.png" alt="FFT Low-Pass" />
            <figcaption>FFT of Low-Pass</figcaption>
          </figure>
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_fft_highpass.png" alt="FFT High-Pass" />
            <figcaption>FFT of High-Pass</figcaption>
          </figure>
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_2/dog_cat/dog_cat_fft_hybrid.png" alt="FFT Hybrid" />
            <figcaption>FFT of Hybrid</figcaption>
          </figure>
        </div>

        <!-- Final Hybrid -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="results/part2_2/batman_joker/batman_joker_hybrid.png" alt="Hybrid Final" />
            <figcaption>Final Hybrid (Batman + Joker)</figcaption>
            <p class="cutoff-note">Cutoff Frequency: ______</p>
          </figure>
        </div>

        <!-- Equation -->
        <div class="equation-box">
          <p>The hybrid image is computed as:</p>
          <p>
            \[
            H(x,y) \;=\; \beta \, L(I_\text{low})(x,y) \;+\; \alpha \, H(I_\text{high})(x,y)
            \]
          </p>
        </div>

        <p class="explainer-text">
          This example demonstrates the full hybrid pipeline. The high-frequency content (Batman) was extracted 
          using a Gaussian high-pass filter, while the low-frequency content (Joker) was preserved with a Gaussian 
          low-pass filter. Their Fourier transforms confirm that the high-pass emphasizes edges and fine details, 
          while the low-pass smooths broader structures. When combined, the resulting hybrid blends the two images, 
          revealing Batman when viewed up close and Joker when viewed from a distance.
        </p>

        <h4 class="section-title">Hybrid Example 2 & 3: Final Results</h4>

        <!-- Hybrid Example 2 -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="results/part2_2/hybrid2/original3.png" alt="Original 3" />
            <figcaption>Original 3</figcaption>
          </figure>
          <figure role="listitem">
            <img src="results/part2_2/hybrid2/original4.png" alt="Original 4" />
            <figcaption>Original 4</figcaption>
          </figure>
          <figure role="listitem">
            <img src="results/part2_2/hybrid2/hybrid.png" alt="Hybrid 2" />
            <figcaption>Hybrid Result</figcaption>
            <p class="cutoff-note">Cutoff Frequency: ______</p>
          </figure>
        </div>

        <!-- Hybrid Example 3 -->
        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="results/part2_2/hybrid3/original5.png" alt="Original 5" />
            <figcaption>Original 5</figcaption>
          </figure>
          <figure role="listitem">
            <img src="results/part2_2/hybrid3/original6.png" alt="Original 6" />
            <figcaption>Original 6</figcaption>
          </figure>
          <figure role="listitem">
            <img src="results/part2_2/hybrid3/hybrid.png" alt="Hybrid 3" />
            <figcaption>Hybrid Result</figcaption>
            <p class="cutoff-note">Cutoff Frequency: ______</p>
          </figure>
        </div>

        <p class="explainer-text">
          For the additional hybrid pairs, only the original images and final results are shown. As with Example 1, 
          each hybrid was built by combining the low-frequency structure of one image with the high-frequency detail 
          of another, producing visuals that depend on viewing distance.
        </p>
      </article>
    </section>
  </main>

  <footer class="footer">
    <p>© 2025 • CS180/280A • Project 2</p>
  </footer>
</body>
</html>
