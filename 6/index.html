<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 5 — Diffusion Models</title>
  <link rel="stylesheet" href="style.css"/>
  <!-- MathJax -->
<script>
  window.MathJax = {
    tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
    svg: { fontCache: 'global' }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>

<body>

<!-- ===== HEADER ===== -->
<header class="site-header">
  <h1>CS180 Project 5: Diffusion Models</h1>
  <p class="subtitle">Eason Wei | CS 180 — Fall 2025</p>
</header>

<!-- ===== SIDEBAR ===== -->

<!-- ===== MAIN CONTENT ===== -->
<main class="container">
   <section class="card" id="part-a">
  <h1 style="font-size: 2.4rem; color: #0c1f33; margin-bottom: 8px;">
    Part A: The Power of Diffusion Models!
  </h1>
  <p style="color: var(--muted); font-size: 1.05rem;">
    Programming Project #5 (proj5) — CS180: Intro to Computer Vision and Computational Photography
  </p>
</section>

  <section id="part0" class="card">
    <h2>Part 0: Setup</h2>

    <h3>Text Prompts Used for Prompt Embeddings</h3>

    <p>
      To explore the behavior of the DeepFloyd diffusion model, I generated prompt embeddings
      for a diverse set of text prompts ranging from highly detailed, imaginative scenes to
      very simple object-level descriptions.
    </p>

    <div class="textbox">
      <ul>
        <li>a giant library floating in the clouds, glowing with jellyfish lights</li>
        <li>a neon samurai standing in a rainy cyberpunk street</li>
        <li>a realistic cat dressed like a king in a Renaissance painting</li>
        <li>a crystal dragon coming out of a frozen waterfall at sunrise</li>
        <li>an astronaut walking inside a huge ancient temple on an alien planet</li>
        <li>a tiny house floating inside a glowing glass bubble</li>
        <li>a giant whale swimming through a sky full of clouds</li>
        <li>a robot painting a sunset on a canvas</li>
        <li>a tree growing on top of a small island floating in space</li>
        <li>a traditional Chinese ink painting of mountains, water, and a small boat</li>
        <li>a cartoon owl sitting on a branch</li>
        <li>a small lantern glowing in the night</li>
        <li>a realistic portrait of a human face</li>
        <li>a small cottage on a hill at sunset</li>
        <li>a high quality photo</li>
        <li>a rocket ship</li>
        <li>a Quinjet</li>
        <li>a pyramid</li>
        <li>an oil painting of an old man</li>
        <li>an oil painting of people around a campfire</li>
        <li>a cat</li>
        <li>a dog</li>
        <li>a landscape of a mountain range</li>
        <li>a skull</li>
        <li>a waterfall</li>
      </ul>
    </div>

    <div class="image-stack">
  <figure>
    <img src="part0_50.png" alt="Complex prompt generations">
    <figcaption>
      Results with <b>50 inference steps</b> (seed = <b>180</b>). Some semantic details are
      underdeveloped or missing due to insufficient denoising.
    </figcaption>
  </figure>
</div>

<div class="image-stack">
  <figure>
    <img src="part0_100.png" alt="Simple prompt generations">
    <figcaption>
      Results with <b>100 inference steps</b> (seed = <b>180</b>). Textures and edges are
      noticeably sharper, and previously missing semantic elements—such as the astronaut
      figure—are clearly formed.
    </figcaption>
  </figure>
</div>

  </section>

 <section id="part1_1" class="card">
  <h2>1.1 — Forward Diffusion Process</h2>

  <p>
    The forward diffusion process gradually corrupts a clean image by adding Gaussian noise.
    Given a clean image \(x_0\), the noisy image \(x_t\) at timestep \(t\) is generated by
    scaling the original signal and injecting noise according to a predefined noise schedule.
  </p>

  <p>
    Let \(\bar{\alpha}_t\) denote the cumulative noise coefficient at timestep \(t\).
    The forward process is defined as:
  </p>

  <p class="math-block">
    \[
      x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1 - \bar{\alpha}_t}\,\varepsilon,
      \quad \varepsilon \sim \mathcal{N}(0, I)
    \]
  </p>

  <p>
    Below is the implementation of the forward diffusion function. The noise coefficient
    \(\bar{\alpha}_t\) is retrieved from <code>alphas_cumprod</code>, and the noise
    \(\varepsilon\) is sampled from a standard Gaussian distribution.
  </p>

  <pre><code class="language-python">
def forward(im, t):
    alpha_bar_t = alphas_cumprod[t].to(im.device).type_as(im)
    eps = torch.randn_like(im)
    im_noisy = torch.sqrt(alpha_bar_t) * im + torch.sqrt(1 - alpha_bar_t) * eps
    return im_noisy
  </code></pre>

  <p>
    As \(t\) increases, \(\bar{\alpha}_t\) decreases, causing the original image content
    to fade while noise dominates. At large timesteps, the image approaches pure Gaussian noise.
  </p>

  <!-- ========================= -->
  <!-- Forward Diffusion Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.1.png" alt="Forward diffusion at different timesteps">
      <figcaption>
        Forward diffusion applied to the Campanile image at increasing timesteps,
        showing progressively stronger noise as \(t\) grows.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_2" class="card">
  <h2>1.2 — Classical Denoising</h2>

  <p>
    I apply
    Gaussian denoising to images corrupted at timesteps
    \(t \in \{250, 500, 750\}\).
  </p>

  <!-- ========================= -->
  <!-- Gaussian Denoising Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.2.png" alt="Gaussian denoising results">
      <figcaption>
        Gaussian-denoised Campanile images at timesteps
        \(t = 250, 500, 750\). The filtering smooths noise but primarily
        introduces blur, failing to restore meaningful structure or sharp edges.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_3" class="card">
  <h2>1.3 — One-Step Denoising</h2>

  <p>
    In one-step denoising, the pretrained diffusion model is used to estimate the
    Gaussian noise present in a noisy image \(x_t\) at timestep \(t\). Given the
    predicted noise \(\hat{\varepsilon}\) and the known noise coefficient
    \(\bar{\alpha}_t\), we can directly recover an estimate of the original clean
    image \(x_0\) in a single step.
  </p>

  <p>
    The reconstruction is performed using the following equation:
  </p>

  <pre><code>
x0_hat = (im_noisy - torch.sqrt(1 - alpha_bar) * noise_est) / torch.sqrt(alpha_bar)
  </code></pre>


  <!-- ========================= -->
  <!-- One-Step Denoising Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.3.png" alt="One-step denoising results">
      <figcaption>
        one can see that the noisier the image, the harder for 1-step denoiser to recover the clean image
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_4" class="card">
  <h2>1.4 — Iterative Denoising</h2>

  <!-- ========================= -->
  <!-- Iterative Denoising Loop -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.4_1.png" alt="Iterative denoising progression">
      <figcaption>
        Iterative denoising progression starting from timestep
        <code>strided_timesteps[10]</code>. The image is shown every 5 denoising steps.
        Noise is gradually removed and semantic structure emerges as the timestep
        decreases.
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Method Comparison -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.4_2.png" alt="Denoising method comparison">
      <figcaption>
        Comparison of denoising methods using the same starting noisy image
        (<code>i_start = 10</code>). From left to right: iterative diffusion denoising,
        one-step denoising, and Gaussian blurring. Iterative denoising produces the
        sharpest result and best preserves global structure, while one-step denoising
        degrades at higher noise levels and Gaussian blurring fails to recover meaningful
        details.
      </figcaption>
    </figure>
  </div>
</section>


  <section id="part1_5" class="card">
  <h2>1.5 — Diffusion Model Sampling</h2>
  <!-- ========================= -->
  <!-- Sampling Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.5.png" alt="Diffusion sampling results">
      <figcaption>
        Five samples generated from pure Gaussian noise using iterative diffusion
        sampling with the prompt <code>"a high quality photo"</code>. Each image
        is produced from a different random noise initialization.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_6" class="card">
  <h2>1.6 — Classifier-Free Guidance (CFG)</h2>

  <p>
    The final guided noise estimate is computed using the following equation:
  </p>

  <pre><code>
noise_est = noise_u + scale * (noise_c - noise_u)
  </code></pre>

  <p>
    Here, <code>scale</code> controls the strength of guidance. In my implementation,
    I use the prompt <code>"a high quality photo"</code> as the conditional input,
    and an empty string <code>""</code> as the unconditional input. Larger guidance
    scales improve prompt adherence at the cost of reduced sample diversity.
  </p>

  <!-- ========================= -->
  <!-- CFG Sampling Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.6.png" alt="CFG sampling results">
      <figcaption>
        Five images generated using classifier-free guidance with
        <code>scale = 7</code>. Compared to unguided sampling, CFG produces sharper
        images with stronger semantic alignment to the prompt
        <code>"a high quality photo"</code>.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_7" class="card">
  <h2>1.7 — Image-to-Image Translation</h2>

  <!-- ========================= -->
  <!-- Campanile Edits -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.png" alt="Campanile image-to-image edits">
      <figcaption>
        Image-to-image translation results for the Campanile image using CFG.
        Edits are generated with starting indices
        <code>[1, 3, 5, 7, 10, 20]</code>  with conditional text prompt "a high quality photo", showing progressively stronger deviations
        from the original image as the noise level increases.
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Test Image 1 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7_1.png" alt="Test image 1 edits">
      <figcaption>

      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Test Image 2 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7_2.png" alt="Test image 2 edits">
      <figcaption>
   
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_7_1" class="card">
  <h2>1.7.1 — Editing Hand-Drawn and Web Images</h2>
  <!-- ========================= -->
  <!-- Web Image Editing -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.1.1.png" alt="Web image edits">
      <figcaption>
        Image-to-image translation applied to a butterfly image downloaded
        from the web. Results are shown for starting indices
        <code>[1, 3, 5, 7, 10, 20]</code>, demonstrating increasing deviation from the
        original image as noise increases.
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Hand-Drawn Image 1 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.1.2.png" alt="Hand-drawn image 1 edits">
      <figcaption>
        Image-to-image translation applied to a hand-drawn emoji image
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Hand-Drawn Image 2 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.1.3.png" alt="Hand-drawn image 2 edits">
      <figcaption>
        Image-to-image translation applied to a second hand-drawn tree image.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_7_2" class="card">
  <h2>1.7.2 — Inpainting</h2>

  <pre><code>
noised_original = forward(original_image, prev_t).half().to(device)
pred_prev_image = mask * pred_prev_image + (1 - mask) * noised_original
  </code></pre>

  <p>
    This ensures that only pixels inside the mask are modified by the diffusion process,
    while all other regions remain faithful to the original image.
  </p>

  <!-- ===== Image Stack 1 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.2.1.png" alt="Original Campanile">
      <figcaption>
        squre mask at the top of campanile
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 2 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.2.2.png" alt="Inpainting mask">
      <figcaption>
        pumpkins with a circular mask
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 3 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.2.3.png" alt="Inpainted Campanile">
      <figcaption>
        sky with bird masked out
      </figcaption>
    </figure>
  </div>

</section>

  <section id="part1_7_3" class="card">
  <h2>1.7.3 — Text-Conditional Image-to-Image Translation</h2>
  <!-- ===== Image Stack 1 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.3.1.png" alt="Rocket ship edits at different noise levels">
      <figcaption>
        prompt: "a rocket ship"
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 2 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.3.2.png" alt="Custom image edit 1">
      <figcaption>
        prompt: "a Quinjet"
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 3 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.3.3.png" alt="Custom image edit 2">
      <figcaption>
        prompt: "a pyramid"
      </figcaption>
    </figure>
  </div>

</section>

  <section id="part1_8" class="card">
  <h2>1.8 — Visual Anagrams</h2>

  <p>
    At each denoising step, I run the diffusion model twice on the <i>same image and
    timestep</i>, but with two different text prompts. One pass uses the image as-is,
    while the other pass uses a vertically flipped version of the image. I then flip
    the second noise estimate back and <b>average the two predicted noises</b> before
    performing the reverse diffusion step.
  </p>

  <p>
    The core implementation is shown below:
  </p>

  <pre><code>
eps1 = uncond_noise1 + scale * (noise1 - uncond_noise1)
eps2 = uncond_noise2 + scale * (noise2 - uncond_noise2)
eps2 = torch.flip(eps2, dims=[2])
eps = (eps1 + eps2) / 2.0
  </code></pre>

  <h3>Visual Anagram Results</h3>

<div class="anagram-pair">
  <figure>
    <img src="oldman_and_fire.png" alt="Old man upright">
    <figcaption>Upright view: <i>an oil painting of an old man</i></figcaption>
  </figure>

  <figure>
    <img src="flip1.png" alt="Campfire flipped">
    <figcaption>Flipped view: <i>an oil painting of people around a campfire</i></figcaption>
  </figure>
</div>

<div class="anagram-pair">
  <figure>
    <img src="whale_and_Quinjet.png" alt="Quinjet upright">
    <figcaption>Upright view: <i>a Quinjet</i></figcaption>
  </figure>

  <figure>
    <img src="flip2.png" alt="Whale flipped">
    <figcaption>Flipped view: <i>a giant whale swimming through a sky full of clouds</i></figcaption>
  </figure>
</div>

<div class="anagram-pair">
  <figure>
    <img src="mountain_and_face.png" alt="Mountains upright">
    <figcaption>Upright view: <i>a landscape of a mountain range</i></figcaption>
  </figure>

  <figure>
    <img src="flip3.png" alt="Face flipped">
    <figcaption>Flipped view: <i>a realistic portrait of a human face</i></figcaption>
  </figure>
</div>

    <section id="part1_9" class="card">
  <h2>1.9 — Hybrid Images</h2>

  <p>
    I run the UNet twice with different text prompts to obtain
    two noise estimates. I then extract low frequencies from the first noise estimate
    and high frequencies from the second, and combine them to form a composite noise
    estimate for the reverse diffusion step.
  </p>

  <pre><code>
low1 = TF.gaussian_blur(eps1, kernel_size=33, sigma=2.0)
low2 = TF.gaussian_blur(eps2, kernel_size=33, sigma=2.0)

high2 = eps2 - low2

eps = low1 + high2
  </code></pre>
      
  <div class="hybrid-row">
    <figure>
      <img src="skull_and_cottage.png" alt="Hybrid of skull and cottage">
      <figcaption>
        Hybrid image combining <i>a skull</i> (low-frequency structure) and
        <i>a small cottage on a hill at sunset</i> (high-frequency details).
        From afar, the skull dominates; up close, the cottage emerges.
      </figcaption>
    </figure>

    <figure>
      <img src="oldman_and_chinese_ink_painting.png" alt="Hybrid of old man and Chinese ink painting">
      <figcaption>
        Hybrid image combining <i>an oil painting of an old man</i> (low-frequency structure) and
        <i>a traditional Chinese ink painting of mountains, water, and a small boat</i>
        (high-frequency details).
      </figcaption>
    </figure>
  </div>
</section>


    <section class="card" id="part-b">
  <h1 style="font-size: 2.4rem; color: #0c1f33; margin-bottom: 8px;">
    Part B: Flow Matching from Scratch!
  </h1>
  <p style="color: var(--muted); font-size: 1.05rem;">
    Programming Project #5 (proj5) — CS180: Intro to Computer Vision and Computational Photography
  </p>
</section>

    <section class="card" id="part-b-1-1-1-2">
  <h2>1.1–1.2 Single-Step Denoising UNet & Noising Process</h2>

  <p>
    I begin by building a single-step denoising UNet that maps a noisy image
    directly back to a clean MNIST digit. Training data is generated by
    synthetically corrupting clean images with Gaussian noise at varying noise
    levels.
  </p>

  <p>
    The figure below shows a single MNIST digit with increasing noise levels,
    illustrating how the input gradually transitions from a clean image to
    near-pure Gaussian noise.
  </p>

  <div class="image-stack">
    <figure>
      <img src="B1.2.png" alt="MNIST noising process">
      <figcaption>
        Noising process applied to a single MNIST digit with increasing noise
        levels σ from left to right.
      </figcaption>
    </figure>
  </div>
</section>
    
    <section class="card" id="part-b-1-2-1">
  <h2>1.2.1 Training</h2>

  <p>
    I train the UNet denoiser to map noisy MNIST digits back to clean images using
    MSE. During training, each image batch is corrupted  with
    Gaussian noise with σ = 0.5.
  </p>

  <!-- ========================= -->
  <!-- Image Stack 1: Loss Curve -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.2.1_1png.png" alt="Training loss curve">
      <figcaption>
        Training loss curve for the single-step denoising UNet over 5 epochs
        (σ = 0.5). The steadily decreasing loss indicates stable convergence.
      </figcaption>
    </figure>
  </div>

  <!-- ===================================== -->
  <!-- Image Stack 2: Results After Epoch 1 -->
  <!-- ===================================== -->
  <div class="image-stack">
    <figure>
      <img src="1.2.1_2.png" alt="Denoising results after epoch 1">
      <figcaption>
        Denoising results on the MNIST test set after the <b>1st epoch</b>.
        the output digits, though blurry, are recognizable indicating the denoising is quite good already afer the first epoch.
      </figcaption>
    </figure>
  </div>

  <!-- ===================================== -->
  <!-- Image Stack 3: Results After Epoch 5 -->
  <!-- ===================================== -->
  <div class="image-stack">
    <figure>
      <img src="1.2.1_3.png" alt="Denoising results after epoch 5">
      <figcaption>
        Denoising results on the MNIST test set after the <b>5th epoch</b>.
        Digits are significantly sharper and closely resemble clean MNIST images.
      </figcaption>
    </figure>
  </div>
</section>

    <section class="card">
  <h3>1.2.2 Out-of-Distribution Testing</h3>

  <div class="image-stack">
    <figure>
      <img src="1.2.3.png"
           alt="Out-of-distribution denoising results across noise levels">
      <figcaption>
        Out-of-distribution denoising results on a fixed MNIST digit (<b>7</b>)
        across noise levels
        <code>σ = 0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0</code> (left to right).
        Although the denoiser was trained only on MNIST digits corrupted with
    <code>σ = 0.5</code>, it generalizes remarkably well to unseen noise levels.
    As the noise level increases, fine-grained details gradually degrade and the
    output becomes blurrier, but the underlying digit structure remains
    recognizable—even at <code>σ = 1.0</code> (pure noise).
      </figcaption>
    </figure>
  </div>
</section>
</main>

<!-- ===== FOOTER ===== -->
<footer>
  <p>© 2025 Eason Wei | UC Berkeley CS180</p>
</footer>

</body>
</html>
