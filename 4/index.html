<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 • Project 2 — Part 1</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="page-header">
    <h1>CS180 — Project 2</h1>
    <h2>Part 1: Fun with Filters</h2>
  </header>

  <main class="container">

    <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2 class="section-title">Part 1</h2>

      <!-- =========== Part 1.1 =========== -->
      <article id="part1-1">
        <h3 class="section-title">1.1 Convolutions from Scratch</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Grayscale Selfie &amp; Derivatives)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_gray.png" />
            <figcaption>Original (Grayscale)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_box_scipy.png" alt="Selfie after convolution (box filter)" />
            <figcaption>After Convolution (Box Filter)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Ix.png" alt="Selfie Lx" />
            <figcaption>Finite Difference: L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Iy.png" alt="Selfie Ly" />
            <figcaption>Finite Difference: L<sub>y</sub></figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          The four nested for-loop implementation of convolution is extremely slow. 
          Efficiency improves significantly when switching to the two for-loop version, 
          where I pad the image with zeros using <code>np.pad</code> based on the kernel size. 
          My self-implemented convolution functions produce the exact same output as the built-in 
          <code>scipy.signal.convolve2d</code>. Compared to the original grayscale selfie, the box-filtered 
          result appears blurred. Additionally, convolving the image with D<sub>x</sub> and D<sub>y</sub> 
          produces the corresponding derivative images shown above.
        </p>

        <details class="code-toggle">
          <summary>Show / Hide my convolution code</summary>

          <div class="code-columns">
            <figure class="code-card">
              <figcaption class="code-title">Four-Loop Convolution (no padding)</figcaption>
<pre class="code-block"><code>def conv2d_four_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    ch, cw = kh // 2, kw // 2
    k   = _flip_kernel(kernel).astype(np.float32, copy=False)
    img = img.astype(np.float32, copy=False)
    out = np.zeros((H, W), dtype=np.float32)

    for i in range(H):
        for j in range(W):
            s = 0.0
            for u in range(-ch, ch + 1):
                for v in range(-cw, cw + 1):
                    ii = i + u
                    jj = j + v
                    ku = ch + u
                    kv = cw + v
                    if 0 &lt;= ii &lt; H and 0 &lt;= jj &lt; W:
                        s += img[ii, jj] * k[ku, kv]
            out[i, j] = s
    return out</code></pre>
            </figure>

            <figure class="code-card">
              <figcaption class="code-title">Two-Loop Convolution (zero padding)</figcaption>
<pre class="code-block"><code>def conv2d_two_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    k = _flip_kernel(kernel)
    ch, cw = kh // 2, kw // 2
    out = np.zeros((H, W), dtype=np.float32)
    padded = np.pad(img, ((ch, ch), (cw, cw)), mode='constant', constant_values=pad_value)
    for i in range(H):
        for j in range(W):
            window = padded[i:i+kh, j:j+kw]
            out[i, j] = np.sum(window * k, dtype=np.float32)
    return out</code></pre>
            </figure>
          </div>
        </details>
      </article>

      <!-- =========== Part 1.2 =========== -->
      <article id="part1-2">
        <h3 class="section-title">1.2 Edge Detection with Finite Differences</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Edge Maps &amp; Gradient)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Ix.png" alt="Lx Gradient" />
            <figcaption>Gradient L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Iy.png" alt="Ly Gradient" />
            <figcaption>Gradient L<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/gradient.png" alt="Gradient Magnitude" />
            <figcaption>Gradient Magnitude</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/binarize_edges.png" alt="Binarized Edges" />
            <figcaption>Binarized Edge Map</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Using the finite difference operators D<sub>x</sub> and D<sub>y</sub>, I computed the partial
          derivatives of the image in both the horizontal and vertical directions. Combining these
          results gives the gradient magnitude map, which highlights edges regardless of orientation.
          By applying a threshold of 0.32~0.34 the gradient magnitude is converted into a binarized edge map, which successfully eliminating most of the background noise
          while outlining the major boundaries in the image.
        </p>
      </article>
    </section>
  </main>

  <footer class="footer">
    <p>© 2025 • CS180/280A • Project 2</p>
  </footer>
</body>
</html>
