<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 • Project 2 — Part 1</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="page-header">
    <h1>CS180 — Project 2</h1>
    <h2>Part 1: Fun with Filters</h2>
  </header>

  <main class="container">

    <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2 class="section-title">Part 1</h2>

      <!-- =========== Part 1.1 =========== -->
      <article id="part1-1">
        <h3 class="section-title">1.1 Convolutions from Scratch</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Grayscale Selfie &amp; Derivatives)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_gray.png" />
            <figcaption>Original (Grayscale)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_box_scipy.png" alt="Selfie after convolution (box filter)" />
            <figcaption>After Convolution (Box Filter)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Ix.png" alt="Selfie Lx" />
            <figcaption>Finite Difference: L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Iy.png" alt="Selfie Ly" />
            <figcaption>Finite Difference: L<sub>y</sub></figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          The four nested for-loop implementation of convolution is extremely slow. 
          Efficiency improves significantly when switching to the two for-loop version, 
          where I pad the image with zeros using <code>np.pad</code> based on the kernel size. 
          My self-implemented convolution functions produce the exact same output as the built-in 
          <code>scipy.signal.convolve2d</code>. Compared to the original grayscale selfie, the box-filtered 
          result appears blurred. Additionally, convolving the image with D<sub>x</sub> and D<sub>y</sub> 
          produces the corresponding derivative images shown above.
        </p>

        <details class="code-toggle">
          <summary>Show / Hide my convolution code</summary>

          <div class="code-columns">
            <figure class="code-card">
              <figcaption class="code-title">Four-Loop Convolution (no padding)</figcaption>
<pre class="code-block"><code>def conv2d_four_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    ch, cw = kh // 2, kw // 2
    k   = _flip_kernel(kernel).astype(np.float32, copy=False)
    img = img.astype(np.float32, copy=False)
    out = np.zeros((H, W), dtype=np.float32)

    for i in range(H):
        for j in range(W):
            s = 0.0
            for u in range(-ch, ch + 1):
                for v in range(-cw, cw + 1):
                    ii = i + u
                    jj = j + v
                    ku = ch + u
                    kv = cw + v
                    if 0 &lt;= ii &lt; H and 0 &lt;= jj &lt; W:
                        s += img[ii, jj] * k[ku, kv]
            out[i, j] = s
    return out</code></pre>
            </figure>

            <figure class="code-card">
              <figcaption class="code-title">Two-Loop Convolution (zero padding)</figcaption>
<pre class="code-block"><code>def conv2d_two_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    k = _flip_kernel(kernel)
    ch, cw = kh // 2, kw // 2
    out = np.zeros((H, W), dtype=np.float32)
    padded = np.pad(img, ((ch, ch), (cw, cw)), mode='constant', constant_values=pad_value)
    for i in range(H):
        for j in range(W):
            window = padded[i:i+kh, j:j+kw]
            out[i, j] = np.sum(window * k, dtype=np.float32)
    return out</code></pre>
            </figure>
          </div>
        </details>
      </article>

      <!-- =========== Part 1.2 =========== -->
      <article id="part1-2">
        <h3 class="section-title">1.2 Edge Detection with Finite Differences</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Edge Maps &amp; Gradient)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Ix.png" alt="Lx Gradient" />
            <figcaption>Gradient L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Iy.png" alt="Ly Gradient" />
            <figcaption>Gradient L<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/gradient.png" alt="Gradient Magnitude" />
            <figcaption>Gradient Magnitude</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/binarize_edges.png" alt="Binarized Edges" />
            <figcaption>Binarized Edge Map</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Using the finite difference operators D<sub>x</sub> and D<sub>y</sub>, I computed the partial
          derivatives of the image in both the horizontal and vertical directions. Combining these
          results gives the gradient magnitude map, which highlights edges regardless of orientation.
          By applying a threshold of 0.32~0.34 the gradient magnitude is converted into a binarized edge map, which successfully eliminating most of the background noise
          while outlining the major boundaries in the image.
        </p>
      </article>

      <!-- =========== Part 1.3 =========== -->
      <article id="part1-3">
        <h3 class="section-title">1.3 Derivative of Gaussian (DoG) Filter</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Gaussian Smoothing &amp; DoG)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/edges_blur.png" alt="Gaussian Blur edge map" />
            <figcaption>Gaussian Blur edge map</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/edges_DoG.png" alt="DoG Gaussian edge map" />
            <figcaption>DoG Gaussian edge map<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/gradient_blur.png" alt="Gaussian Blur gradient" />
            <figcaption>Gaussian Blur gradient<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/gradient_DoG.png" alt="DoG Gaussian Blur gradient" />
            <figcaption>DoG Gaussian Blur gradient</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Applying finite difference operators directly on the original image (as in Part 1.2) often produces noisy edge maps.
          By first smoothing the image with a Gaussian filter (as in Part 1.3), noise is reduced and edges appear sharper and more coherent. 
          The Derivative of Gaussian (DoG) filters combine smoothing and differentiation into a single operation, producing results equivalent to 
          applying Gaussian blur followed by convolution with D<sub>x</sub> or D<sub>y</sub>.
          
          Interestingly, while both approaches yield visually identical binarized edge maps at a threshold of 0.2, their gradient magnitudes differ 
          slightly. The DoG results appear somewhat brighter compared to the Gaussian-blur-then-differentiate approach. This discrepancy is likely due to 
          subtle differences in numerical precision and the order of operations: in the DoG method, the derivative is embedded in the Gaussian kernel itself, 
          whereas in the two-step method, smoothing and differentiation are performed sequentially
        </p>
      </article>

<!-- =========== Part 2.1 =========== -->
<article id="part2-1">
  <h3 class="section-title">2.1 Image Sharpening</h3>

  <!-- Top Row -->
  <h4 class="section-title">Taj Mahal Example</h4>
  <div class="image-strip" role="list">
    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/0_original.png" alt="Original Taj" />
      <figcaption>Original</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/1_blurred.png" alt="Blurred Taj" />
      <figcaption>Blurred</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/2_highfreq_vis.png" alt="High-Freq Taj" />
      <figcaption>High-Frequency</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/taj_demo/3_sharp_two_step.png" alt="Sharpened Taj" />
      <figcaption>Sharpened</figcaption>
    </figure>
  </div>

  <p class="explainer-text">
    For the Taj Mahal image, unsharp masking enhances the architectural details while keeping the
    overall structure intact. The blurred version shows the loss of sharp edges, while the
    high-frequency component isolates fine details. Adding these back to the blurred image
    recovers a crisp, sharpened result.
  </p>

  <!-- Bottom Row -->
  <h4 class="section-title">Pyramid Example</h4>
  <div class="image-strip" role="list">
    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/0_original.png" alt="Original Pyramid" />
      <figcaption>Original</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/05_demo_blurred.png" alt="Blurred Pyramid" />
      <figcaption>Blurred</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/02_highfreq_vis.png" alt="High-Freq Pyramid" />
      <figcaption>High-Frequency</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/pyramid/06_demo_resharpened.png" alt="Resharpened Pyramid" />
      <figcaption>Resharpened</figcaption>
    </figure>
  </div>

  <p class="explainer-text">
    In the Pyramid example, the sharpening process clearly emphasizes the stone textures. The
    blurred image loses edge definition, but the extracted high-pass details restore the fine
    structure once added back. This demonstrates how unsharp masking can effectively recover
    clarity even after deliberate blurring.
  </p>
</article>
    </section>
  </main>

  <footer class="footer">
    <p>© 2025 • CS180/280A • Project 2</p>
  </footer>
</body>
</html>
