<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 4: Neural Radiance Fields (NeRF)</title>
  <link rel="stylesheet" href="style.css"/>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <!-- ===== HEADER ===== -->
  <header class="site-header">
    <h1>Project 4: Neural Radiance Fields (NeRF)</h1>
    <p class="subtitle">Eason Wei | CS 180 / 280A – Fall 2025</p>
  </header>

  <!-- ===== SIDEBAR NAVIGATION ===== -->
  <nav class="sidebar">
    <ul>
      <li><a href="#intro">Introduction</a></li>
      <li><a href="#part0">Part 0 — Data Capture & Calibration</a></li>
      <li><a href="#part01">0.1 — Camera Calibration</a></li>
      <li><a href="#part02">0.2 — Object Capture</a></li>
      <li><a href="#part03">0.3 — Pose Estimation</a></li>
      <li><a href="#part04">0.4 — Dataset Packaging</a></li>
      <li><a href="#part1">Part 1 — 2D Neural Field</a></li>
      <li><a href="#part2_1">Part 2.1 — Create Rays</a></li>
      <li><a href="#part2_2">Part 2.2 — Sampling</a></li>
      <li><a href="#part2_3">Part 2.3 — Dataloading</a></li>
      <li><a href="#part2_4">Part 2.4 — NeRF Model</a></li>
      <li><a href="#part2_5">Part 2.5 — Volume Rendering</a></li>
      <li><a href="#part2_6">Part 2.6 — Own Data Training</a></li>
    </ul>
  </nav>

  <!-- ===== MAIN CONTENT ===== -->
  <main class="container">

    <!-- ========================= -->
    <!-- INTRODUCTION -->
    <!-- ========================= -->
    <section id="intro" class="card">
      <h2>Introduction</h2>
      <p>
        In this project, I implement a full NeRF pipeline—from capturing a real object with a phone,
        to calibrating the camera, estimating poses, training a 2D neural field, and finally training
        a complete 3D Neural Radiance Field capable of synthesizing novel views.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 0 OVERVIEW -->
    <!-- ========================= -->
    <section id="part0" class="card">
      <h2>Part 0 — Camera Calibration & 3D Scanning Pipeline</h2>
      <p>
        I begin by printing a 6-tag ArUco grid, capturing 50 calibration images, and running
        <code>cv2.calibrateCamera</code> to recover the intrinsic matrix \(K\). I then capture 50
        images of my Labubu figure, detect an ArUco tag in each frame, and estimate all camera-to-world
        poses using the PnP algorithm.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.1 CAMERA CALIBRATION -->
    <!-- ========================= -->
    <section id="part01" class="card">
      <h2>0.1 — Camera Calibration</h2>

      <p>
        I printed a 6-tag ArUco grid and captured 50 high-resolution images. They were originally in
        <code>.heic</code> format, so I batch-converted them to <code>.jpg</code> for processing. Each
        printed marker measured 0.57 units wide, which I used to define the world coordinates of all tag
        corners.
      </p>

      <p>
        Using <code>cv2.aruco.detectMarkers</code>, I detected all visible markers and applied a mask to
        ensure consistent indexing across frames. Then I called <code>cv2.calibrateCamera</code> to recover
        the intrinsic matrix \(K\) and distortion coefficients.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.2 OBJECT CAPTURE -->
    <!-- ========================= -->
    <section id="part02" class="card">
      <h2>0.2 — Object Capture</h2>
      <p>
        I captured ~50 images of my Labubu figure beside a single ArUco tag. I kept exposure fixed and
        rotated around the object to maximize multi-view coverage.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.3 POSE ESTIMATION -->
    <!-- ========================= -->
    <section id="part03" class="card">
      <h2>0.3 — Pose Estimation (PnP)</h2>
      <p>For each image, I detect the tag corners and solve the PnP problem.</p>

      <p class="math-block">
        \[
        x_i \sim K [R|t] 
        \begin{bmatrix} X_i \\ 1 \end{bmatrix}
        \]
      </p>

      <p class="math-block">
        \[
        R = \mathrm{Rodrigues}(\mathbf{r})
        \]
      </p>

      <p class="math-block">
        \[
        \mathrm{w2c} =
        \begin{bmatrix}
        R & t\\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p class="math-block">
        \[
        \mathrm{c2w} =
        \begin{bmatrix}
        R^\top & -R^\top t \\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <div class="image-stack">
        <figure>
          <img src="viser1.png" alt="Viser Plot 1">
        </figure>

        <figure>
          <img src="viser2.png" alt="Viser Plot 2">
        </figure>
      </div>
    </section>

    <!-- ========================= -->
    <!-- 0.4 PACKAGING -->
    <!-- ========================= -->
    <section id="part04" class="card">
      <h2>0.4 — Dataset Packaging</h2>
      <p>
        I load all images, undistort them using the calibrated intrinsics, compute the resized focal
        length, and then package everything into <code>my_data.npz</code>.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 1 -->
    <!-- ========================= -->
    <section id="part1" class="card">
      <h2>Part 1 — Neural Field for 2D Image Reconstruction</h2>

      <p>
        I train a coordinate-based MLP to reconstruct a 2D RGB image from normalized pixel coordinates.
      </p>

      <p class="math-block">
        \[
        F_\theta(x, y) \to \text{RGB}
        \]
      </p>

      <div class="textbox">
        <h3>Neural Field Architecture</h3>
        <p><b>Positional Encoding:</b> 10 frequencies (also compared with 2)</p>
        <p><b>MLP:</b> width 256 (also compared with 64), two hidden layers</p>
        <p><b>Output:</b> Sigmoid RGB</p>
      </div>

      <div class="image-stack">
        <figure><img src="wolf_progress.png"></figure>
        <figure><img src="wolf_grid.png"></figure>
        <figure><img src="flowers_progress.png"></figure>
        <figure><img src="flowers_grid.png"></figure>
        <figure><img src="psnrs.png"></figure>
      </div>
    </section>

    <!-- ========================= -->
    <!-- PART 2.1 -->
    <!-- ========================= -->
    <section id="part2_1" class="card">
      <h2>Part 2.1 — Create Rays from Cameras</h2>
      <p>Convert pixel coordinates to 3D world rays using intrinsics and camera poses.</p>

      <p class="math-block">
        \[
        \mathbf{r}(t) = \mathbf{o} + t\,\mathbf{d}
        \]
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 2.2 -->
    <!-- ========================= -->
    <section id="part2_2" class="card">
      <h2>Part 2.2 — Sampling Points Along Rays</h2>

      <p>
        For each ray, I sample 64 stratified points between a near/far bound.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 2.3 -->
    <!-- ========================= -->
    <section id="part2_3" class="card">
      <h2>Part 2.3 — Dataloading</h2>

      <ul>
        <li><b>Global sampling</b>: sample rays across all images</li>
        <li><b>Single-camera sampling</b>: debugging mode</li>
      </ul>

      <div class="image-stack">
        <figure><img src="viser3.png"></figure>
        <figure><img src="viser4.png"></figure>
      </div>
    </section>

    <!-- ========================= -->
    <!-- PART 2.4 -->
    <!-- ========================= -->
    <section id="part2_4" class="card">
      <h2>Part 2.4 — Neural Radiance Field</h2>
      <p>Standard NeRF architecture with positional encoding and skip connections.</p>
    </section>

    <!-- ========================= -->
    <!-- PART 2.5 — VOLUME RENDERING -->
    <!-- ========================= -->
    <section id="part2_5" class="card">
      <h2>Part 2.5 — Volume Rendering</h2>

      <p class="math-block">
      \[
      C(\mathbf{r}) = \sum_{i=1}^{N} T_i \alpha_i \mathbf{c}_i,
      \quad
      T_i = \exp\!\Big(-\sum_{j<i} \sigma_j \delta_j\Big),
      \quad
      \alpha_i = 1 - \exp(-\sigma_i \delta_i)
      \]
      </p>

      <div class="image-stack">
        <figure><img src="training_progress.png"></figure>
        <figure><img src="psnr_curves.png"></figure>
      </div>

      <!-- GIF + Replay -->
      <div class="gif-wrapper">
        <img id="lego-gif" src="lego_rotation.gif" alt="Lego Rotation GIF">
        <button id="replay-btn">Replay GIF</button>
      </div>
    </section>

    <!-- ========================= -->
    <!-- PART 2.6 -->
    <!-- ========================= -->
    <section id="part2_6" class="card">
      <h2>Part 2.6 — Training on My Own Data</h2>

      <div class="textbox">
        <h3>Hyperparameters</h3>
        <ul>
          <li>10k iterations</li>
          <li>Batch size 8192</li>
          <li>NeRF: 8 layers, width 256</li>
        </ul>
      </div>

      <div class="image-grid-3col">
        <img src="val_0200.png">
        <img src="val_1500.png">
        <img src="val_2500.png">
        <img src="val_5000.png">
        <img src="val_8000.png">
        <img src="val_10000.png">
      </div>

      <div class="image-grid-2col">
        <img src="psnr_labubu_curve.png">
        <img src="loss_curve.png">
      </div>

      <div class="gif-container">
        <img src="results/my_nerf_training/orbit.gif" alt="orbit gif">
      </div>
    </section>

  </main>

  <!-- ===== FOOTER ===== -->
  <footer>
    <p>© 2025 Eason Wei | UC Berkeley CS180 – Neural Radiance Fields</p>
  </footer>

  <!-- ===== GIF REPLAY SCRIPT ===== -->
  <script>
    document.getElementById("replay-btn").onclick = function () {
      const gif = document.getElementById("lego-gif");
      const base = gif.src.split("?")[0];
      gif.src = base + "?t=" + Date.now();  // force reload
    };
  </script>

</body>
</html>
