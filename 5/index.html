<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 4: Neural Radiance Fields (NeRF)</title>
  <link rel="stylesheet" href="style.css"/>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <!-- ===== HEADER ===== -->
  <header class="site-header">
    <h1>Project 4: Neural Radiance Fields (NeRF)</h1>
    <p class="subtitle">Eason Wei | CS 180 / 280A – Fall 2025</p>
  </header>

  <!-- ===== SIDEBAR NAVIGATION ===== -->
  <nav class="sidebar">
    <ul>
      <li><a href="#intro">Introduction</a></li>

      <!-- Part 0 -->
      <li><a href="#part0">Part 0 — Data Capture & Calibration</a></li>
      <li><a href="#part01">0.1 — Camera Calibration</a></li>
      <li><a href="#part02">0.2 — Object Capture</a></li>
      <li><a href="#part03">0.3 — Pose Estimation</a></li>
      <li><a href="#part04">0.4 — Dataset Packaging</a></li>

      <!-- Future Parts -->
      <li><a href="#part1">Part 1 — 2D Neural Field</a></li>
      <li><a href="#part2">Part 2 — Ray Sampling</a></li>
      <li><a href="#part3">Part 3 — NeRF Training</a></li>
      <li><a href="#part4">Part 4 — Novel View Synthesis</a></li>
      <li><a href="#results">Results & GIFs</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
    </ul>
  </nav>

  <!-- ===== MAIN CONTENT ===== -->
  <main class="container">

    <!-- ========================= -->
    <!-- INTRODUCTION -->
    <!-- ========================= -->
    <section id="intro" class="card">
      <h2>Introduction</h2>
      <p>
        In this project, I implement a full NeRF pipeline—from capturing a real object with a phone,
        to calibrating the camera, estimating poses, training a 2D neural field, and finally training
        a complete 3D Neural Radiance Field capable of synthesizing novel views.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 0 OVERVIEW -->
    <!-- ========================= -->
    <section id="part0" class="card">
      <h2>Part 0 — Camera Calibration & 3D Scanning Pipeline</h2>
      <p>
        I begin by printing a 6-tag ArUco grid, capturing 50 calibration images, and running
        <code>cv2.calibrateCamera</code> to recover the intrinsic matrix \(K\). I then capture 50
        images of my Labubu figure, detect an ArUco tag in each frame, and estimate all camera-to-world
        poses using the PnP algorithm.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.1 CAMERA CALIBRATION -->
    <!-- ========================= -->
    <section id="part01" class="card">
      <h2>0.1 — Camera Calibration</h2>

      <p>
        I printed a 6-tag ArUco grid and captured 50 high-resolution images. Since they were originally
        in <code>.heic</code> format, I batch-converted them into <code>.jpg</code> for processing. Each
        printed marker measured 0.57 units wide, which I used to define the 3D coordinates of all tag
        corners.
      </p>

      <p>
        Using <code>cv2.aruco.detectMarkers</code>, I detected all visible markers and applied a mask to
        ensure consistent indexing across frames. Finally, I used <code>cv2.calibrateCamera</code> to
        recover the camera’s intrinsic matrix \(K\) and distortion coefficients.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.2 OBJECT CAPTURE -->
    <!-- ========================= -->
    <section id="part02" class="card">
      <h2>0.2 — Object Capture</h2>

      <p>
        I captured ~50 images of my Labubu figure beside a single ArUco tag. The camera remained
        10–20 cm away, ensuring the object filled roughly half the frame. I kept exposure
        consistent, avoided motion blur, and varied the angle around a circular path to maximize
        multi-view coverage.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.3 POSE ESTIMATION -->
    <!-- ========================= -->
    <section id="part03" class="card">
      <h2>0.3 — Pose Estimation (PnP)</h2>

      <p>
        For each image, I detect the tag, extract its 2D corners, and solve for camera extrinsics
        using <code>cv2.solvePnP</code>. The projection model follows:
      </p>

      <p class="math-block">
        \[
        x_i \sim K [R|t] 
        \begin{bmatrix} X_i \\ 1 \end{bmatrix}
        \]
      </p>

      <p>
        The rotation vector is converted using Rodrigues:
      </p>

      <p class="math-block">
        \[
        R = \mathrm{Rodrigues}(\mathbf{r})
        \]
      </p>

      <p>
        World-to-camera:
      </p>

      <p class="math-block">
        \[
        \mathrm{w2c} =
        \begin{bmatrix}
        R & t\\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        And camera-to-world:
      </p>

      <p class="math-block">
        \[
        \mathrm{c2w} =
        \begin{bmatrix}
        R^\top & -R^\top t \\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        Because NeRF uses a different coordinate convention, I apply a diagonal flip:
      </p>

      <p class="math-block">
        \[
        D = \mathrm{diag}(1,-1,-1)
        \]
      </p>

      <p>
        Below are the Viser-rendered camera frustums:
      </p>

      <div class="image-stack">
        <figure>
          <img src="viser1.png" alt="Viser Plot 1">
          <figcaption>Dome-like coverage — dense sampling of azimuth angles.</figcaption>
        </figure>

        <figure>
          <img src="viser2.png" alt="Viser Plot 2">
          <figcaption>Full 360° pose sweep ensures strong multi-view parallax.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ========================= -->
    <!-- 0.4 DATASET PACKAGING -->
    <!-- ========================= -->
    <section id="part04" class="card">
      <h2>0.4 — Dataset Packaging</h2>
      <p>
  In this step, I load the 50 resized Labubu images and their camera poses from Part&nbsp;0.3, then
  undistort each image using the scaled intrinsic matrix \(K\) and the distortion coefficients from
  the original calibration. This produces a clean, distortion-free set of inputs for NeRF.
</p>

<p>
  I then split the 50 images into <b>train/val/test</b> sets using a 70/15/15 ratio. The split is
  done over image indices with a fixed random seed, and each subset contains both the undistorted
  RGB images and their corresponding camera-to-world matrices.
</p>

<p>
  The focal length is computed from the resized intrinsics as
  \( f = (K_{00} + K_{11}) / 2 \), and all data—images, poses, intrinsics, and focal—is packaged
  into <code>my_data.npz</code> for NeRF training.
</p>

      <p>
        This single file is used for every remaining stage of the NeRF pipeline.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PLACEHOLDER SECTIONS -->
    <!-- ========================= -->
    <section id="part1" class="card">
  <h2>Part 1 — Neural Field for 2D Image Reconstruction</h2>

  <p>
    In this part, I train a coordinate-based neural network to reconstruct a 2D image. 
    The model learns a continuous mapping
  </p>

  <p class="math-block">
    \[
      F_{\theta}(x, y) \rightarrow \text{RGB},
    \]
  </p>

  <p>
    where each pixel is represented by normalized \((x, y)\) coordinates in \([0,1]^2\).
    With positional encoding and a small MLP, the network gradually learns to reproduce 
    the full-resolution image.
  </p>

  <!-- ─────────────────────────────────────────────── -->
  <!-- Concise Architecture Box -->
  <!-- ─────────────────────────────────────────────── -->
  <div style="
    background: rgba(20,25,35,0.8);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 14px 18px;
    margin: 20px 0;
    font-size: 0.93rem;
  ">
    <h3 style="color: var(--accent); margin-bottom: 8px;">Neural Field Architecture</h3>

    <p><b>Input:</b> 2D pixel coordinate \((x, y)\)</p>

    <p><b>Positional Encoding:</b> 10 frequency bands  
       <span style="color: var(--muted);">(also compared with 2)</span>
    </p>

    <p><b>MLP:</b></p>
    <ul style="margin-left: 18px;">
      <li>Hidden width: 256 
        <span style="color: var(--muted);">(also tested 64)</span>
      </li>
      <li>Two Linear + ReLU layers</li>
      <li>Final Linear → Sigmoid (RGB)</li>
    </ul>

    <p><b>Loss:</b> MSE <b>Optimizer:</b> Adam (lr = 1e−2)</p>
  </div>

  <p>
    Below are examples of the reconstruction quality at different training iterations.
  </p>

  <!-- ─────────────────────────────────────────────── -->
  <!-- Five-image placeholder gallery -->
  <!-- ─────────────────────────────────────────────── -->
  <div class="image-stack">
    <figure>
      <img src="wolf_progress.png" alt="Original">
    </figure>
    <p>
  During training, the MLP learns a continuous mapping from 2D coordinates → RGB values using batches of randomly 
  sampled pixels. At early iterations (e.g., 50–100), the network captures only coarse color blobs because it first 
  fits the low-frequency structure of the image. As training progresses, higher-frequency details emerge: edges become 
  sharper, textures appear, and colors stabilize. By 1000–2000 iterations, the model converges to a smooth, 
  high-fidelity reconstruction that closely matches the original image. This progression highlights how neural fields 
  naturally learn images from coarse-to-fine detail through gradient descent.
</p>

    <figure>
      <img src="wolf_grid.png" alt="Iter 50">
    </figure>
    <p>
  The hyperparameter sweep clearly shows how positional frequency and network width affect reconstruction quality. 
  With only <b>2 frequency bands</b>, the model cannot represent high-frequency detail, producing overly smooth and 
  blurry results even with a wider MLP. Increasing to <b>10 frequencies</b> dramatically sharpens edges and textures, 
  enabling the network to reproduce fine details in the fur and background. Width also matters: the narrow 
  <b>64-unit</b> MLP struggles with capacity, introducing graininess and losing subtle shading, while the 
  <b>256-unit</b> version produces smoother colors and more faithful contours. Overall, <b>high frequency + large width</b> 
  yields the most accurate reconstruction, demonstrating that spatial detail is controlled primarily by positional 
  encoding, while network width governs representational capacity.
  </p>


    <figure>
      <img src="flowers_progress.png" alt="Iter 250">
      <figcaption class="notes">reconstruction for flower image</figcaption>
    </figure>

    <figure>
      <img src="flowers_grid.png" alt="Iter 1000">
      <figcaption class="notes">reconstruction using a 2x2 grid of results</figcaption>
    </figure>

    <figure>
      <img src="psnr_flower.png" alt="Final Reconstruction">
      <figcaption class="notes">psnrs for flower image</figcaption>
    </figure>
  </div>

</section>

    <!-- ====================================================== -->
<!-- ===================== PART 2 ========================= -->
<!-- ====================================================== -->
  <section id="part2_1" class="card">
  <h2>Part 2.1 — Create Rays from Cameras</h2>

  <p>
    In this step, I convert each pixel in an image into a <b>3D camera ray</b> that NeRF can train on.
    For every pixel coordinate \((u, v)\), I first back-project it through the intrinsic matrix 
    <code>K</code> to obtain a point at unit depth in <i>camera coordinates</i>. I then transform this 
    point into <i>world coordinates</i> using the camera-to-world matrix <code>c2w</code>, which gives 
    me the ray origin (the camera center) and a normalized ray direction. This produces one ray per pixel — 
    the fundamental input representation for NeRF.
  </p>

  <p class="math-block">
    \[
    \mathbf{r}(t) = \mathbf{o} + t\,\mathbf{d}
    \]
  </p>

  <p>
    I verify the implementation by round-tripping a test point through <code>c2w</code> and its inverse,
    and I also visualize a small pixel grid's ray directions to ensure they are consistent with the image 
    geometry. These checks confirm that my camera-to-ray conversion is correct.
  </p>
</section>

  <!-- ===== Part 2.2 ===== -->
  <section id="part2_2" class="card">
  <h2>Part 2.2 — Sampling Points Along Rays</h2>

  <p>
    After generating camera rays, the next step is to sample 3D points along each ray so the NeRF 
    MLP can predict color and density at those positions. I first randomly select an image, then 
    randomly choose pixel indices and convert them into ray origins \(\mathbf{o}\) and directions 
    \(\mathbf{d}\). This produces a batch of rays and their corresponding ground-truth RGB values.
  </p>

  <p>
    For each ray, I sample <b>64 points</b> between a near and far bound (2.0 → 6.0). 
    I use <b>stratified sampling</b>, which jitters each interval to produce unbiased Monte-Carlo samples 
    and reduce aliasing. This is the standard NeRF sampling approach.
  </p>

  <p class="math-block">
    \[
    t_i \sim \text{Uniform}(t_{i,\text{lower}},\, t_{i,\text{upper}}), \qquad
    \mathbf{x}_i = \mathbf{o} + t_i\,\mathbf{d}.
    \]
  </p>

  <p>
    The result is a tensor of sampled 3D points of shape \((N, 64, 3)\), and corresponding distances 
    \(t_i\). These samples are later fed into the NeRF network and volume renderer. I also confirm the 
    implementation by printing example rays, directions, and their first few sampled 3D points.
  </p>
</section>

  <!-- ===== Part 2.3 ===== -->
  <h3>2.3 — Putting the Dataloading All Together</h3>
  <p>
    Placeholder content — packaging rays, RGB targets, batching, shuffling,
    and preparing the dataset for training.
  </p>

  <!-- ===== Part 2.4 ===== -->
  <h3>2.4 — Neural Radiance Field (MLP)</h3>
  <p>
    Placeholder content — positional encoding, MLP width/depth, sigma/color heads,
    and activation selection.
  </p>

  <!-- ===== Part 2.5 ===== -->
  <h3>2.5 — Volume Rendering</h3>
  <p>
    Placeholder content — converting sampled densities into alpha weights,
    computing transmittance, integrating color along rays, and producing
    differentiable RGB predictions.
  </p>

  <!-- ===== Part 2.6 ===== -->
  <h3>2.6 — Training with Your Own Data</h3>
  <p>
    Placeholder content — training loop, batching strategy, PSNR curves,
    and convergence behavior on your Labubu dataset.
  </p>
</section>


</main>

<!-- ===== FOOTER ===== -->
<footer>
  <p>© 2025 Eason Wei | UC Berkeley CS180 – Neural Radiance Fields</p>
</footer>
</body>
</html>
