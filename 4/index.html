<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>CS180 • Project 2 — Part 1</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="page-header">
    <h1>CS180 — Project 2</h1>
    <h2>Part 1: Fun with Filters</h2>
  </header>

  <main class="container">

    <!-- ===================== Part 1 ===================== -->
    <section id="part1">
      <h2 class="section-title">Part 1</h2>

      <!-- =========== Part 1.1 =========== -->
      <article id="part1-1">
        <h3 class="section-title">1.1 Convolutions from Scratch</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Grayscale Selfie &amp; Derivatives)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_gray.png" />
            <figcaption>Original (Grayscale)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_box_scipy.png" alt="Selfie after convolution (box filter)" />
            <figcaption>After Convolution (Box Filter)</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Ix.png" alt="Selfie Lx" />
            <figcaption>Finite Difference: L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_1/selfie_Iy.png" alt="Selfie Ly" />
            <figcaption>Finite Difference: L<sub>y</sub></figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          The four nested for-loop implementation of convolution is extremely slow. 
          Efficiency improves significantly when switching to the two for-loop version, 
          where I pad the image with zeros using <code>np.pad</code> based on the kernel size. 
          My self-implemented convolution functions produce the exact same output as the built-in 
          <code>scipy.signal.convolve2d</code>. Compared to the original grayscale selfie, the box-filtered 
          result appears blurred. Additionally, convolving the image with D<sub>x</sub> and D<sub>y</sub> 
          produces the corresponding derivative images shown above.
        </p>

        <details class="code-toggle">
          <summary>Show / Hide my convolution code</summary>

          <div class="code-columns">
            <figure class="code-card">
              <figcaption class="code-title">Four-Loop Convolution (no padding)</figcaption>
<pre class="code-block"><code>def conv2d_four_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    ch, cw = kh // 2, kw // 2
    k   = _flip_kernel(kernel).astype(np.float32, copy=False)
    img = img.astype(np.float32, copy=False)
    out = np.zeros((H, W), dtype=np.float32)

    for i in range(H):
        for j in range(W):
            s = 0.0
            for u in range(-ch, ch + 1):
                for v in range(-cw, cw + 1):
                    ii = i + u
                    jj = j + v
                    ku = ch + u
                    kv = cw + v
                    if 0 &lt;= ii &lt; H and 0 &lt;= jj &lt; W:
                        s += img[ii, jj] * k[ku, kv]
            out[i, j] = s
    return out</code></pre>
            </figure>

            <figure class="code-card">
              <figcaption class="code-title">Two-Loop Convolution (zero padding)</figcaption>
<pre class="code-block"><code>def conv2d_two_forloops(img, kernel, pad_value: float = 0.0):
    H, W = img.shape
    kh, kw = kernel.shape
    k = _flip_kernel(kernel)
    ch, cw = kh // 2, kw // 2
    out = np.zeros((H, W), dtype=np.float32)
    padded = np.pad(img, ((ch, ch), (cw, cw)), mode='constant', constant_values=pad_value)
    for i in range(H):
        for j in range(W):
            window = padded[i:i+kh, j:j+kw]
            out[i, j] = np.sum(window * k, dtype=np.float32)
    return out</code></pre>
            </figure>
          </div>
        </details>
      </article>

      <!-- =========== Part 1.2 =========== -->
      <article id="part1-2">
        <h3 class="section-title">1.2 Edge Detection with Finite Differences</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Edge Maps &amp; Gradient)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Ix.png" alt="Lx Gradient" />
            <figcaption>Gradient L<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/Iy.png" alt="Ly Gradient" />
            <figcaption>Gradient L<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/gradient.png" alt="Gradient Magnitude" />
            <figcaption>Gradient Magnitude</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_2/binarize_edges.png" alt="Binarized Edges" />
            <figcaption>Binarized Edge Map</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Using the finite difference operators D<sub>x</sub> and D<sub>y</sub>, I computed the partial
          derivatives of the image in both the horizontal and vertical directions. Combining these
          results gives the gradient magnitude map, which highlights edges regardless of orientation.
          By applying a threshold of 0.32~0.34 the gradient magnitude is converted into a binarized edge map, which successfully eliminating most of the background noise
          while outlining the major boundaries in the image.
        </p>
      </article>

      <!-- =========== Part 1.3 =========== -->
      <article id="part1-3">
        <h3 class="section-title">1.3 Derivative of Gaussian (DoG) Filter</h3>

        <!-- Results Row -->
        <h4 class="section-title">Results (Gaussian Smoothing &amp; DoG)</h4>

        <div class="image-strip" role="list">
          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/edges_blur.png" alt="Gaussian Blur edge map" />
            <figcaption>Gaussian Blur edge map</figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/edges_DoG.png" alt="DoG Gaussian edge map" />
            <figcaption>DoG Gaussian edge map<sub>x</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/gradient_blur.png" alt="Gaussian Blur gradient" />
            <figcaption>Gaussian Blur gradient<sub>y</sub></figcaption>
          </figure>

          <figure role="listitem">
            <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part1_3/gradient_DoG.png" alt="DoG Gaussian Blur gradient" />
            <figcaption>DoG Gaussian Blur gradient</figcaption>
          </figure>
        </div>

        <p class="explainer-text">
          Applying finite difference operators directly on the original image (as in Part 1.2) often produces noisy edge maps.
          By first smoothing the image with a Gaussian filter (as in Part 1.3), noise is reduced and edges appear sharper and more coherent. 
          The Derivative of Gaussian (DoG) filters combine smoothing and differentiation into a single operation, producing results equivalent to 
          applying Gaussian blur followed by convolution with D<sub>x</sub> or D<sub>y</sub>.
          
          Interestingly, while both approaches yield visually identical binarized edge maps at a threshold of 0.2, their gradient magnitudes differ 
          slightly. The DoG results appear somewhat brighter compared to the Gaussian-blur-then-differentiate approach. This discrepancy is likely due to 
          subtle differences in numerical precision and the order of operations: in the DoG method, the derivative is embedded in the Gaussian kernel itself, 
          whereas in the two-step method, smoothing and differentiation are performed sequentially
        </p>
      </article>

      <!-- =========== Part 2.1 =========== -->
<article id="part2-1">
  <h3 class="section-title">2.1 Image Sharpening</h3>

  <!-- Results Grid -->
  <h4 class="section-title">Results (Original, Blurred, High-Pass, and Sharpened)</h4>

  <div class="image-strip wrap" role="list">
    <!-- Top Row -->
    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/0_original.png" alt="Original Image 1" />
      <figcaption>Original</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/1_blurred.png" alt="Blurred Image 1" />
      <figcaption>Blurred</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/2_highfreq_vis.png" alt="High-Pass Image 1" />
      <figcaption>High_Frequency</figcaption>
    </figure>

    <figure role="listitem">
      <img src="https://raw.githubusercontent.com/eason-w322/CS180-Project-2/main/results/part2_1/3_sharp_two_step.png" alt="Sharpened Image 1" />
      <figcaption>Sharpened</figcaption>
    </figure>

    <!-- Bottom Row -->
    <figure role="listitem">
      <img src="REPLACE_WITH_URL_original2.png" alt="Original Image 2" />
      <figcaption>Original 2</figcaption>
    </figure>

    <figure role="listitem">
      <img src="REPLACE_WITH_URL_blur2.png" alt="Blurred Image 2" />
      <figcaption>Blurred 2</figcaption>
    </figure>

    <figure role="listitem">
      <img src="REPLACE_WITH_URL_highpass2.png" alt="High-Pass Image 2" />
      <figcaption>High-Pass 2</figcaption>
    </figure>

    <figure role="listitem">
      <img src="REPLACE_WITH_URL_sharpened2.png" alt="Sharpened Image 2" />
      <figcaption>Sharpened 2</figcaption>
    </figure>
  </div>

  <!-- Explanation -->
  <p class="explainer-text">
    In this section, I applied the unsharp masking technique for image sharpening. Each example 
    shows the original image, its Gaussian-blurred version, the extracted high-pass component, and 
    the final sharpened result. By adjusting the blending weight of the high frequencies, the 
    sharpened images appear more crisp while preserving the overall structure. In some cases, 
    oversharpening can amplify noise, which highlights the importance of choosing filter sizes 
    and scaling factors carefully.
  </p>
</article>
    </section>
  </main>

  <footer class="footer">
    <p>© 2025 • CS180/280A • Project 2</p>
  </footer>
</body>
</html>
