<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 4: Neural Radiance Fields (NeRF)</title>
  <link rel="stylesheet" href="style.css"/>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <!-- ===== HEADER ===== -->
  <header class="site-header">
    <h1>Project 4: Neural Radiance Fields (NeRF)</h1>
    <p class="subtitle">Eason Wei | CS 180 / 280A – Fall 2025</p>
  </header>

  <!-- ===== SIDEBAR NAVIGATION ===== -->
  <nav class="sidebar">
    <ul>
      <li><a href="#intro">Introduction</a></li>

      <!-- Part 0 -->
      <li><a href="#part0">Part 0 — Data Capture & Calibration</a></li>
      <li><a href="#part01">0.1 — Camera Calibration</a></li>
      <li><a href="#part02">0.2 — Object Capture</a></li>
      <li><a href="#part03">0.3 — Pose Estimation</a></li>
      <li><a href="#part04">0.4 — Dataset Packaging</a></li>

      <!-- Future Parts -->
      <li><a href="#part1">Part 1 — 2D Neural Field</a></li>
      <li><a href="#part2">Part 2 — Ray Sampling</a></li>
      <li><a href="#part3">Part 3 — NeRF Training</a></li>
      <li><a href="#part4">Part 4 — Novel View Synthesis</a></li>
      <li><a href="#results">Results & GIFs</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
    </ul>
  </nav>

  <!-- ===== MAIN CONTENT ===== -->
  <main class="container">

    <!-- ========================= -->
    <!-- INTRODUCTION -->
    <!-- ========================= -->
    <section id="intro" class="card">
      <h2>Introduction</h2>
      <p>
        In this project, I implement a full NeRF pipeline—from capturing a real object with a phone,
        to calibrating the camera, estimating poses, training a 2D neural field, and finally training
        a complete 3D Neural Radiance Field capable of synthesizing novel views.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 0 OVERVIEW -->
    <!-- ========================= -->
    <section id="part0" class="card">
      <h2>Part 0 — Camera Calibration & 3D Scanning Pipeline</h2>
      <p>
        I begin by printing a 6-tag ArUco grid, capturing 50 calibration images, and running
        <code>cv2.calibrateCamera</code> to recover the intrinsic matrix \(K\). I then capture 50
        images of my Labubu figure, detect an ArUco tag in each frame, and estimate all camera-to-world
        poses using the PnP algorithm.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.1 CAMERA CALIBRATION -->
    <!-- ========================= -->
    <section id="part01" class="card">
      <h2>0.1 — Camera Calibration</h2>

      <p>
        I printed a 6-tag ArUco grid and captured 50 high-resolution images. Since they were originally
        in <code>.heic</code> format, I batch-converted them into <code>.jpg</code> for processing. Each
        printed marker measured 0.57 units wide, which I used to define the 3D coordinates of all tag
        corners.
      </p>

      <p>
        Using <code>cv2.aruco.detectMarkers</code>, I detected all visible markers and applied a mask to
        ensure consistent indexing across frames. Finally, I used <code>cv2.calibrateCamera</code> to
        recover the camera’s intrinsic matrix \(K\) and distortion coefficients.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.2 OBJECT CAPTURE -->
    <!-- ========================= -->
    <section id="part02" class="card">
      <h2>0.2 — Object Capture</h2>

      <p>
        I captured ~50 images of my Labubu figure beside a single ArUco tag. The camera remained
        10–20 cm away, ensuring the object filled roughly half the frame. I kept exposure
        consistent, avoided motion blur, and varied the angle around a circular path to maximize
        multi-view coverage.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.3 POSE ESTIMATION -->
    <!-- ========================= -->
    <section id="part03" class="card">
      <h2>0.3 — Pose Estimation (PnP)</h2>

      <p>
        For each image, I detect the tag, extract its 2D corners, and solve for camera extrinsics
        using <code>cv2.solvePnP</code>. The projection model follows:
      </p>

      <p class="math-block">
        \[
        x_i \sim K [R|t] 
        \begin{bmatrix} X_i \\ 1 \end{bmatrix}
        \]
      </p>

      <p>
        The rotation vector is converted using Rodrigues:
      </p>

      <p class="math-block">
        \[
        R = \mathrm{Rodrigues}(\mathbf{r})
        \]
      </p>

      <p>
        World-to-camera:
      </p>

      <p class="math-block">
        \[
        \mathrm{w2c} =
        \begin{bmatrix}
        R & t\\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        And camera-to-world:
      </p>

      <p class="math-block">
        \[
        \mathrm{c2w} =
        \begin{bmatrix}
        R^\top & -R^\top t \\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        Because NeRF uses a different coordinate convention, I apply a diagonal flip:
      </p>

      <p class="math-block">
        \[
        D = \mathrm{diag}(1,-1,-1)
        \]
      </p>

      <p>
        Below are the Viser-rendered camera frustums:
      </p>

      <div class="image-stack">
        <figure>
          <img src="viser_cam_plot1.png" alt="Viser Plot 1">
          <figcaption>Dome-like coverage — dense sampling of azimuth angles.</figcaption>
        </figure>

        <figure>
          <img src="viser_cam_plot2.png" alt="Viser Plot 2">
          <figcaption>Full 360° pose sweep ensures strong multi-view parallax.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ========================= -->
    <!-- 0.4 DATASET PACKAGING -->
    <!-- ========================= -->
    <section id="part04" class="card">
      <h2>0.4 — Dataset Packaging</h2>

      <p>
        All valid resized images, their matched camera-to-world matrices, and the scaled intrinsic
        matrix \(K\) are saved into a compact <code>.npz</code> file:
      </p>

      <pre><code>
np.savez(
    "data/poses_and_images.npz",
    c2ws=c2ws,
    images=valid_imgs,
    K=K
)
      </code></pre>

      <p>
        This single file is used for every remaining stage of the NeRF pipeline.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PLACEHOLDER SECTIONS -->
    <!-- ========================= -->
    <section id="part1" class="card">
      <h2>Part 1 — 2D Neural Field</h2>
      <p>Placeholder content — to be filled with positional encoding, MLP, PSNR curves, etc.</p>
    </section>

    <section id="part2" class="card">
      <h2>Part 2 — Ray Sampling</h2>
      <p>Placeholder content — stratified sampling, hierarchical sampling, etc.</p>
    </section>

    <section id="part3" class="card">
      <h2>Part 3 — NeRF Training</h2>
      <p>Placeholder content — MLP architecture, loss, training curves.</p>
    </section>

    <section id="part4" class="card">
      <h2>Part 4 — Novel View Synthesis</h2>
      <p>Placeholder content — renderings, orbit GIFs, spherical GIFs.</p>
    </section>

    <section id="results" class="card">
      <h2>Results & GIFs</h2>
      <p>Placeholder content — final GIFs and comparisons.</p>
    </section>

    <section id="conclusion" class="card">
      <h2>Conclusion</h2>
      <p>Placeholder content — reflections and analysis.</p>
    </section>

  </main>

  <!-- ===== FOOTER ===== -->
  <footer>
    <p>© 2025 Eason Wei | UC Berkeley CS180 – Neural Radiance Fields</p>
  </footer>
</body>
</html>
