<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 3: Image Warping and Mosaicing</title>
  <link rel="stylesheet" href="style.css"/>
</head>

<body>
  <!-- ===== HEADER ===== -->
  <header class="site-header">
    <h1>Project 3: [Auto]Stitching Photo Mosaics</h1>
    <p class="subtitle">Eason Wei | CS 180 / 280A – Fall 2025</p>
  </header>

  <!-- ===== SIDEBAR NAVIGATION ===== -->
  <nav class="sidebar">
    <ul>
      <li><a href="#partA1">A.1 – Shoot and Digitize Pictures</a></li>
      <li><a href="#partA2">A.2 – Recover Homographies</a></li>
      <li><a href="#partA3">A.3 – Warp the Images</a></li>
      <li><a href="#partA4">A.4 – Blend into a Mosaic</a></li>
      <!-- Part B links -->
      <li><a href="#partB1">B.1 – Harris & ANMS</a></li>
      <li><a href="#partB2">B.2 – Descriptors</a></li>
      <li><a href="#partB3">B.3 – Feature Matching</a></li>
      <li><a href="#partB4">B.4 – RANSAC & Mosaics</a></li>
    </ul>
  </nav>

  <!-- ===== MAIN CONTENT ===== -->
  <main class="container">

    <!-- ===== Overview Section ===== -->
    <section id="intro" class="card">
      <h2>Overview</h2>
      <p>
        In this project, I explore <b>image warping</b> and <b>mosaicing</b> to create seamless panoramas
        from multiple photographs. The goal is to recover homographies between images, warp them using
        nearest-neighbor and bilinear interpolation, and blend them into smooth mosaics.
      </p>
    </section>

    <!-- ===== Part A.1 Section ===== -->
    <section id="partA1" class="card">
      <h2>A.1 – Shoot and Digitize Pictures</h2>
      <p>
        I captured multiple photo sets by rotating the camera about a fixed center of projection
        to create projective transformations suitable for mosaicing. Each image overlaps the next
        by a portion to ensure reliable feature matching and homography estimation.
      </p>

      <div class="image-stack">
        <figure>
          <img src="set1_projective_images.png" alt="Set 1 - Piano With Projective Transformation">
          <figcaption>Set 1 - Piano With Projective Transformation</figcaption>
        </figure>

        <figure>
          <img src="set2_projective_images.png" alt="Set 2 - View with Projective Transformation">
          <figcaption>Set 2 - View with Projective Transformation</figcaption>
        </figure>

        <figure>
          <img src="set3_projective_images.png" alt="Set 1 - Image 3">
          <figcaption>Set 3 - Berkeley with Projective Transformation</figcaption>
        </figure>
      </div>
    </section>

    <!-- ===== Part A.2 Section ===== -->
    <section id="partA2" class="card">
      <h2>A.2 – Recover Homographies</h2>
      <p>
        Before warping the images into alignment, I first need to recover the parameters of the 
        <b>projective transformation</b> (homography) that maps points from one image to another. 
        The goal is to find a 3×3 matrix <b>H</b> such that \( p' = H p \), where \(p = [x, y, 1]^T\) 
        and \(p' = [u, v, 1]^T\).
      </p>

      <h3>1. Homography Definition</h3>
      <p>
        Since the last entry can be fixed to 1, the matrix has eight unknowns:
      </p>

      <p class="math-block">
        \[
        H =
        \begin{bmatrix}
        h_1 & h_2 & h_3 \\
        h_4 & h_5 & h_6 \\
        h_7 & h_8 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        Expanding the homogeneous relation \(p' = H p\) yields:
      </p>

      <p class="math-block">
        \[
        \begin{cases}
        u = \frac{h_1 x + h_2 y + h_3}{h_7 x + h_8 y + 1}, \\[6pt]
        v = \frac{h_4 x + h_5 y + h_6}{h_7 x + h_8 y + 1}.
        \end{cases}
        \]
      </p>

      <p>
        Multiplying through by the denominators gives two linear equations per correspondence:
      </p>

      <p class="math-block">
        \[
        \begin{cases}
        x h_1 + y h_2 + h_3 - u x h_7 - u y h_8 = u,\\[6pt]
        x h_4 + y h_5 + h_6 - v x h_7 - v y h_8 = v.
        \end{cases}
        \]
      </p>

      <h3>2. Constructing the Linear System</h3>
      <p>
        Each correspondence contributes two rows to a matrix <b>A</b> and a right-hand side vector <b>b</b>:
      </p>

      <p class="math-block">
        \[
        A_i =
        \begin{bmatrix}
        x_i & y_i & 1 & 0 & 0 & 0 & -u_i x_i & -u_i y_i\\
        0 & 0 & 0 & x_i & y_i & 1 & -v_i x_i & -v_i y_i
        \end{bmatrix},\quad
        b_i =
        \begin{bmatrix}
        u_i \\ v_i
        \end{bmatrix}
        \]
      </p>

      <p>
        Stacking all n point correspondences, we form the full system:
      </p>

      <p class="math-block">
        \[
        A\,h = b,\quad
        A \in \mathbb{R}^{2n \times 8},\;
        h = [h_1,h_2,h_3,h_4,h_5,h_6,h_7,h_8]^T.
        \]
      </p>

      <p>
        For four points, the system can be solved exactly, but for stability and robustness, 
        I use <b>more than four correspondences</b> and solve the overdetermined system using 
        the least-squares solution:
      </p>

      <p class="math-block">
        \[
        h = (A^T A)^{-1} A^T b.
        \]
      </p>

      <p>
        Finally, the solution vector \(h\) is reshaped into the 3×3 matrix \(H\) by appending the bottom row \([0,0,1]\):
      </p>

      <p class="math-block">
        \[
        H =
        \begin{bmatrix}
        h_1 & h_2 & h_3\\
        h_4 & h_5 & h_6\\
        h_7 & h_8 & 1
        \end{bmatrix}.
        \]
      </p>

      <h3>3. Visualizing Point Correspondences</h3>
      <p>
        To verify the quality of the selected correspondences, I plotted both images side by side and 
        connected matching feature points using <b>red lines</b>. I manually chose 
        <b>eight pairs</b> of distinctive features such as piano corners, window edges, and structural intersections. 
        The visualization clearly demonstrates consistent geometry between the two images, confirming that the 
        computed homography accurately models the perspective transformation.
      </p>

      <figure style="text-align: center; margin: 20px 0;">
        <img src="piano1_to_piano2_correspondences_ls.png" alt="piano1 to piano2 correspondences" style="max-width: 90%; border-radius: 10px; border: 1px solid var(--border); box-shadow: 0 0 10px rgba(0,0,0,0.4);">
        <figcaption style="color: var(--muted); margin-top: 8px;">
          piano1 → piano2 Correspondences 
        </figcaption>
      </figure>

      <h3>4. Recovered Homography Matrix</h3>
      <p>
        The estimated homography matrix \(H\) for the piano1 → piano2 transformation is:
      </p>

      <p class="math-block">
        \[
        H =
        \begin{bmatrix}
        1.9173 & 0.0029 & -1088.9868 \\[4pt]
        0.3469 & 1.5827 & -380.8842 \\[4pt]
        0.0005 & -0.000005 & 1.0000
        \end{bmatrix}
        \]
      </p>

      <!-- MathJax for LaTeX equations -->
      <script>
        window.MathJax = {
          tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
          svg: {fontCache: 'global'}
        };
      </script>
      <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
      </script>
    </section>

    <!-- ===== Part A.3 Section ===== -->
    <section id="partA3" class="card">
      <h2>A.3 – Warp the Images</h2>
      <p>
        Using the recovered homography matrix \(H\), I warped each image into the reference frame 
        through <b>inverse warping</b>. This approach maps every pixel in the target image to 
        its corresponding coordinate in the source image, preventing holes in the output.
      </p>

      <!-- ========================= -->
      <!-- A.3.1 Image Warping -->
      <!-- ========================= -->
      <h3>A.3.1 – Image Warping</h3>

      <p>
        Two interpolation methods were implemented from scratch:
      </p>

      <ul>
        <li><b>Nearest Neighbor:</b> rounds source coordinates to the nearest integer pixel.</li>
        <li><b>Bilinear:</b> interpolates along the <i>x</i>-axis first, then the <i>y</i>-axis 
            using a weighted average of four neighbors.</li>
      </ul>

      <p>
        Both use inverse mapping defined by:
      </p>

      <p class="math-block">
        \[
        \begin{bmatrix}x \\ y \\ 1\end{bmatrix}
        = H^{-1}
        \begin{bmatrix}x' \\ y' \\ 1\end{bmatrix}
        \]
      </p>

      <div style="display: flex; flex-direction: column; gap: 20px;">
        <div>
          <p><b>Nearest Neighbor Sampling:</b></p>
          <pre><code class="language-python">
x_nn, y_nn = int(round(x)), int(round(y))
if 0 <= x_nn < w and 0 <= y_nn < h:
    warped[y_out, x_out] = im[y_nn, x_nn]
          </code></pre>
        </div>

        <div>
          <p><b>Bilinear Interpolation:</b></p>
          <pre><code class="language-python">
x0, y0 = int(np.floor(x)), int(np.floor(y))
dx, dy = x - x0, y - y0

I_top = (1-dx)*im[y0, x0] + dx*im[y0, x0+1]
I_bottom = (1-dx)*im[y0+1, x0] + dx*im[y0+1, x0+1]
warped[y_out, x_out] = (1-dy)*I_top + dy*I_bottom
          </code></pre>
        </div>
      </div>

      <figure style="text-align:center; margin:24px 0;">
        <img src="piano_middle_to_piano_right.png"
             alt="Warp result Nearest Neighbor vs Bilinear"
             style="max-width:90%; border-radius:10px; border:1px solid var(--border);
                    box-shadow:0 0 10px rgba(0,0,0,0.4);">
        <figcaption style="color:var(--muted); margin-top:8px;">
          Comparison of Nearest Neighbor (12.26 s) and Bilinear (24.61 s) Warps
        </figcaption>
      </figure>

      <p>
        The Nearest Neighbor method executes faster but introduces blocky edges and aliasing. 
        Bilinear interpolation, though roughly twice as slow, yields smoother and more realistic results.
      </p>

      <!-- ========================= -->
      <!-- A.3.2 Rectification -->
      <!-- ========================= -->
      <h3>A.3.2 – Image Rectification</h3>

      <p>
        To validate the homography and warping pipeline, I performed <b>rectification</b> — 
        mapping slanted rectangular planes into a fronto-parallel view. For each case, 
        I clicked the four corner points of an oblique rectangle and 
        mapped them to a rectangular grid.
      </p>

      <p>
        The resulting homography straightens the perspective, confirming correct inverse 
        warping and interpolation behavior.
      </p>

      <figure style="text-align:center; margin:20px 0;">
        <img src="tv_rectified_bilinear.png"
             alt="Rectification before_and_after"
             style="max-width:85%; border-radius:10px; border:1px solid var(--border);
                    box-shadow:0 0 10px rgba(0,0,0,0.4); margin-bottom:10px;">
      
        <img src="box_rectified_bilinear.png"
             alt="Rectification before_and_after"
             style="max-width:85%; border-radius:10px; border:1px solid var(--border);
                    box-shadow:0 0 10px rgba(0,0,0,0.4);">
      </figure>
    </section>

    <!-- ===== Part A.4 Section ===== -->
    <section id="partA4" class="card">
      <h2>A.4 – Blend into a Mosaic</h2>
      <p>
        In <b>Part A.4</b>, the blending process creates a seamless mosaic by first warping all three
        piano images—left, middle, and right—into a <b>common coordinate system</b> defined by
        the middle image. Using the precomputed homographies
        \(H_{left \rightarrow mid}\) and \(H_{right \rightarrow mid}\),
        each image (and its binary mask) is projected onto a large
        <b>global canvas</b> using my <code>warpImageBilinear(im, H)</code> function.
      </p>

      <p>
        Once all warped images share the same size \((H, W)\), each mask is converted into a
        <b>soft alpha mask</b> using a distance transform and Gaussian blur to create smooth
        fall-offs along edges. This prevents visible seams and ensures gradual transitions
        between overlapping regions.
      </p>

      <p>
        These smoothed masks are normalized and used as pixel-wise blending weights in the formula:
      </p>

      <p class="math-block" style="text-align:center; margin:24px 0;">
        \[
        \text{blend}(x,y) =
        \frac{\sum_i I_i(x,y)\,\alpha_i(x,y)}{\sum_i \alpha_i(x,y)}
        \]
      </p>

      <p>
        This weighted blending ensures that overlapping regions are <b>averaged proportionally</b>
        rather than overwritten. Finally, the resulting mosaic is <b>auto-cropped</b> to remove
        black borders, producing a clean, naturally blended panorama that combines all three views
        into one continuous image.
      </p>

      <!-- ========================= -->
      <!-- Mosaic Results -->
      <!-- ========================= -->
      <div class="image-stack">
        <figure>
          <img src="piano_mosaic.png" alt="Mosaic1">
          <figcaption>Piano Mosaic</figcaption>
        </figure>

        <figure>
          <img src="view_mosaic.png" alt="Mosaic2">
          <figcaption>View Mosaic</figcaption>
        </figure>

        <figure>
          <img src="berkeley_mosaic.png" alt="Mosaic3">
          <figcaption>Berkeley Mosaic</figcaption>
        </figure>
      </div>
    </section>

    <!-- ===== Part B.1 – Harris & ANMS ===== -->
    <section id="partB1" class="card">
      <h2>B.1 – Harris Corner Detection & Adaptive Non-Maximal Suppression (ANMS)</h2>
      <p class="subtitle">
        <p>
  I detect single-scale Harris corners using <code>harris.py</code>. To reduce clutter from densely clustered responses,
  I increase <code>min_distance</code> from <code>1</code> to <code>5</code> so nearby peaks don’t all get selected.
  Next, I implement <b>Adaptive Non-Maximal Suppression (ANMS)</b>. For each corner \(i\) with response \(R_i\),
  I compute its suppression radius \(r_i\) as the smallest distance to any neighboring corner \(j\) whose response is
  at least \(0.9\,R_i\). Distances are computed with the provided <code>dist2</code> helper.
  I then sort corners by \(r_i\) in descending order and keep the top set, yielding a spatially well-distributed subset.
  The figures below show overlays <b>before</b> and <b>after</b> ANMS.
</p>
      </p>

      <div class="b1-grid">
        <!-- Left column: Harris only -->
        <div class="b1-col">
          <h3>Before ANMS (Harris only)</h3>

          <figure class="b1-figure">
            <img src="soda1_harris_before_anms.png" alt="Harris corners overlay — Image 1">
          </figure>

          <figure class="b1-figure">
            <img src="soda2_harris_before_anms.png" alt="Harris corners overlay — Image 2">
          </figure>

          <figure class="b1-figure">
            <img src="soda3_harris_before_anms.png" alt="Harris corners overlay — Image 3">
          </figure>
        </div>

        <!-- Right column: After ANMS -->
        <div class="b1-col">
          <h3>After ANMS (selected corners)</h3>

          <figure class="b1-figure">
            <img src="soda1_harris_after_anms.png" alt="ANMS-selected corners — Image 1">
          </figure>

          <figure class="b1-figure">
            <img src="soda2_harris_after_anms.png" alt="ANMS-selected corners — Image 2">
          </figure>

          <figure class="b1-figure">
            <img src="soda3_harris_after_anms.png" alt="ANMS-selected corners — Image 3">
          </figure>
        </div>
      </div>
    </section>

    <!-- Optional placeholders for B.2–B.4 (to be filled next) -->
    <section id="partB2" class="card">
  <h2>B.2 – Feature Descriptor Extraction</h2>

  <p class="subtitle">

  <p>
    I first crop a 40×40 window around the corner and apply a Gaussian blur (low-pass filter).
    This <em>suppresses noise</em> and small misalignments, so when I later downsample the window,
    the descriptor is smoother and more robust (anti-aliasing).
  </p>

  <p>
    I downsample the blurred 40×40 window to <b>8×8</b>, then <em>subtract the mean</em>
    (removes brightness offset) and <em>divide by the standard deviation</em>
    (normalizes contrast), which makes descriptors comparable across lighting changes and
    stabilizes distance metrics.
  </p>

  <ol>
    <li>Crop 40×40 window around each corner.</li>
    <li>Apply Gaussian blur (e.g., σ≈2).</li>
    <li>Downsample to 8×8.</li>
    <li>Bias/gain normalize: <code>(patch - mean) / (std + ε)</code>.</li>
  </ol>

  <div class="image-stack">
    <figure>
      <img src="feature_descriptors.png" alt="Descriptor example: 40×40 window → blurred → 8×8 normalized patch">
    </figure>
  </div>

</section>
    <section id="partB3" class="card">
  <h2>B.3 – Feature Matching</h2>

  <p class="subtitle">
  <p>
    For a descriptor \( \mathbf{f} \) in Image&nbsp;1, I find its nearest neighbor distance \(d_1\) and
    second-nearest \(d_2\) among Image&nbsp;2 descriptors. I accept a match if the
    <b>Lowe ratio</b> \(d_1/d_2 < 0.52\). This favors distinctive matches and rejects ambiguous ones.
    (Descriptors are mean/variance normalized, so L2 is a meaningful similarity measure.)
  </p>

  <ol>
    <li>Compute all L2 distances from each Image&nbsp;1 descriptor to Image&nbsp;2 descriptors.</li>
    <li>Record the best (\(d_1\)) and second-best (\(d_2\)) distances.</li>
    <li>Apply ratio test: keep matches with \(d_1/d_2 < 0.52\).</li>
  </ol>

  <div class="image-stack">
    <figure>
      <img src="descriptor_pair_gallery.png" alt="Descriptor example: 40×40 window → blurred → 8×8 normalized patch">
    </figure>
  </div>

      <div class="image-stack">
    <figure>
      <img src="colored_feature_matches.png" alt="Descriptor example: 40×40 window → blurred → 8×8 normalized patch">
    </figure>
  </div>

</section>
    <section id="partB4" class="card">
  <h2>B.4 – RANSAC for Robust Homography</h2>

  <p class="subtitle">
    I estimate a robust homography with <b>4-point RANSAC</b> and then reuse my Part A pipeline
    (bilinear inverse warping + blending) to build mosaics.
  </p>

  <p>
    In each iteration, I randomly sample <b>4</b> matches from <code>match_features</code>, compute a candidate
    homography \(\hat H\) (DLT), and count inliers: correspondences whose transfer error in Image&nbsp;2
    is below a small pixel threshold \(\epsilon\).
    After <b>2000</b> iterations I keep the model with the most inliers and
    <b>re-estimate</b> the final \(H\) by least squares on all inliers. Finally, I warp with bilinear
    interpolation and stitch the images to produce the panorama.
  </p>

  <ol>
    <li>Repeat 2000×: sample 4 matches → compute \(\hat H\) → count inliers (error &lt; \(\epsilon\)).</li>
    <li>Keep \(\hat H^*\) with the <b>max inliers</b>.</li>
    <li>Refit <b>final</b> \(H\) on those inliers and mosaic with the Part A code.</li>
  </ol>

  <!-- Six very-wide panoramas stacked (each set: manual vs automatic) -->
  <div class="image-stack">
    <!-- Set 1 -->
    <figure>
      <img src="soda_mosaic.png" alt="Set 1 — Manual stitching (non-auto)">
      <figcaption>Soda — Manual stitching (non-auto)</figcaption>
    </figure>
    <figure>
      <img src="soda_auto_mosaic.png" alt="Set 1 — Automatic stitching (RANSAC + features)">
      <figcaption>Soda — Automatic stitching (RANSAC + features)</figcaption>
    </figure>

    <!-- Set 2 -->
    <figure>
      <img src="assets/b4/set2_manual.png" alt="Set 2 — Manual stitching (non-auto)">
      <figcaption>Set 2 — Manual stitching (non-auto)</figcaption>
    </figure>
    <figure>
      <img src="assets/b4/set2_auto.png" alt="Set 2 — Automatic stitching (RANSAC + features)">
      <figcaption>Set 2 — Automatic stitching (RANSAC + features)</figcaption>
    </figure>

    <!-- Set 3 -->
    <figure>
      <img src="assets/b4/set3_manual.png" alt="Set 3 — Manual stitching (non-auto)">
      <figcaption>Set 3 — Manual stitching (non-auto)</figcaption>
    </figure>
    <figure>
      <img src="assets/b4/set3_auto.png" alt="Set 3 — Automatic stitching (RANSAC + features)">
      <figcaption>Set 3 — Automatic stitching (RANSAC + features)</figcaption>
    </figure>
  </div>

  <p class="notes">
    Deliverables: implement 4-point RANSAC, compare manual vs automatic stitching; show ≥3 automatic mosaics.
  </p>
</section>

  </main>

  <!-- ===== FOOTER ===== -->
  <footer>
    <p>© 2025 Eason Wei | UC Berkeley CS180 – Image Mosaicing</p>
  </footer>
</body>
</html>
