<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 4: Neural Radiance Fields (NeRF)</title>
  <link rel="stylesheet" href="style.css"/>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
  <!-- ===== HEADER ===== -->
  <header class="site-header">
    <h1>Project 4: Neural Radiance Fields (NeRF)</h1>
    <p class="subtitle">Eason Wei | CS 180 / 280A – Fall 2025</p>
  </header>

  <!-- ===== SIDEBAR NAVIGATION ===== -->
  <nav class="sidebar">
    <ul>
      <li><a href="#intro">Introduction</a></li>

      <!-- Part 0 -->
      <li><a href="#part0">Part 0 — Data Capture & Calibration</a></li>
      <li><a href="#part01">0.1 — Camera Calibration</a></li>
      <li><a href="#part02">0.2 — Object Capture</a></li>
      <li><a href="#part03">0.3 — Pose Estimation</a></li>
      <li><a href="#part04">0.4 — Dataset Packaging</a></li>

      <!-- Future Parts -->
      <li><a href="#part1">Part 1 — 2D Neural Field</a></li>
      <li><a href="#part2">Part 2 — Ray Sampling</a></li>
      <li><a href="#part3">Part 3 — NeRF Training</a></li>
      <li><a href="#part4">Part 4 — Novel View Synthesis</a></li>
      <li><a href="#results">Results & GIFs</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
    </ul>
  </nav>

  <!-- ===== MAIN CONTENT ===== -->
  <main class="container">

    <!-- ========================= -->
    <!-- INTRODUCTION -->
    <!-- ========================= -->
    <section id="intro" class="card">
      <h2>Introduction</h2>
      <p>
        In this project, I implement a full NeRF pipeline—from capturing a real object with a phone,
        to calibrating the camera, estimating poses, training a 2D neural field, and finally training
        a complete 3D Neural Radiance Field capable of synthesizing novel views.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PART 0 OVERVIEW -->
    <!-- ========================= -->
    <section id="part0" class="card">
      <h2>Part 0 — Camera Calibration & 3D Scanning Pipeline</h2>
      <p>
        I begin by printing a 6-tag ArUco grid, capturing 50 calibration images, and running
        <code>cv2.calibrateCamera</code> to recover the intrinsic matrix \(K\). I then capture 50
        images of my Labubu figure, detect an ArUco tag in each frame, and estimate all camera-to-world
        poses using the PnP algorithm.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.1 CAMERA CALIBRATION -->
    <!-- ========================= -->
    <section id="part01" class="card">
      <h2>0.1 — Camera Calibration</h2>

      <p>
        I printed a 6-tag ArUco grid and captured 50 high-resolution images. Since they were originally
        in <code>.heic</code> format, I batch-converted them into <code>.jpg</code> for processing. Each
        printed marker measured 0.57 units wide, which I used to define the 3D coordinates of all tag
        corners.
      </p>

      <p>
        Using <code>cv2.aruco.detectMarkers</code>, I detected all visible markers and applied a mask to
        ensure consistent indexing across frames. Finally, I used <code>cv2.calibrateCamera</code> to
        recover the camera’s intrinsic matrix \(K\) and distortion coefficients.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.2 OBJECT CAPTURE -->
    <!-- ========================= -->
    <section id="part02" class="card">
      <h2>0.2 — Object Capture</h2>

      <p>
        I captured ~50 images of my Labubu figure beside a single ArUco tag. The camera remained
        10–20 cm away, ensuring the object filled roughly half the frame. I kept exposure
        consistent, avoided motion blur, and varied the angle around a circular path to maximize
        multi-view coverage.
      </p>
    </section>

    <!-- ========================= -->
    <!-- 0.3 POSE ESTIMATION -->
    <!-- ========================= -->
    <section id="part03" class="card">
      <h2>0.3 — Pose Estimation (PnP)</h2>

      <p>
        For each image, I detect the tag, extract its 2D corners, and solve for camera extrinsics
        using <code>cv2.solvePnP</code>. The projection model follows:
      </p>

      <p class="math-block">
        \[
        x_i \sim K [R|t] 
        \begin{bmatrix} X_i \\ 1 \end{bmatrix}
        \]
      </p>

      <p>
        The rotation vector is converted using Rodrigues:
      </p>

      <p class="math-block">
        \[
        R = \mathrm{Rodrigues}(\mathbf{r})
        \]
      </p>

      <p>
        World-to-camera:
      </p>

      <p class="math-block">
        \[
        \mathrm{w2c} =
        \begin{bmatrix}
        R & t\\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        And camera-to-world:
      </p>

      <p class="math-block">
        \[
        \mathrm{c2w} =
        \begin{bmatrix}
        R^\top & -R^\top t \\ 0 & 1
        \end{bmatrix}
        \]
      </p>

      <p>
        Because NeRF uses a different coordinate convention, I apply a diagonal flip:
      </p>

      <p class="math-block">
        \[
        D = \mathrm{diag}(1,-1,-1)
        \]
      </p>

      <p>
        Below are the Viser-rendered camera frustums:
      </p>

      <div class="image-stack">
        <figure>
          <img src="viser1.png" alt="Viser Plot 1">
          <figcaption>Dome-like coverage — dense sampling of azimuth angles.</figcaption>
        </figure>

        <figure>
          <img src="viser2.png" alt="Viser Plot 2">
          <figcaption>Full 360° pose sweep ensures strong multi-view parallax.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ========================= -->
    <!-- 0.4 DATASET PACKAGING -->
    <!-- ========================= -->
    <section id="part04" class="card">
      <h2>0.4 — Dataset Packaging</h2>
      <p>
  In this step, I load the 50 resized Labubu images and their camera poses from Part&nbsp;0.3, then
  undistort each image using the scaled intrinsic matrix \(K\) and the distortion coefficients from
  the original calibration. This produces a clean, distortion-free set of inputs for NeRF.
</p>

<p>
  I then split the 50 images into <b>train/val/test</b> sets using a 70/15/15 ratio. The split is
  done over image indices with a fixed random seed, and each subset contains both the undistorted
  RGB images and their corresponding camera-to-world matrices.
</p>

<p>
  The focal length is computed from the resized intrinsics as
  \( f = (K_{00} + K_{11}) / 2 \), and all data—images, poses, intrinsics, and focal—is packaged
  into <code>my_data.npz</code> for NeRF training.
</p>

      <p>
        This single file is used for every remaining stage of the NeRF pipeline.
      </p>
    </section>

    <!-- ========================= -->
    <!-- PLACEHOLDER SECTIONS -->
    <!-- ========================= -->
    <section id="part1" class="card">
  <h2>Part 1 — Neural Field for 2D Image Reconstruction</h2>

  <p>
    In this part, I train a coordinate-based neural network to reconstruct a 2D image. 
    The model learns a continuous mapping
  </p>

  <p class="math-block">
    \[
      F_{\theta}(x, y) \rightarrow \text{RGB},
    \]
  </p>

  <p>
    where each pixel is represented by normalized \((x, y)\) coordinates in \([0,1]^2\).
    With positional encoding and a small MLP, the network gradually learns to reproduce 
    the full-resolution image.
  </p>

  <!-- ─────────────────────────────────────────────── -->
  <!-- Concise Architecture Box -->
  <!-- ─────────────────────────────────────────────── -->
  <div style="
    background: rgba(20,25,35,0.8);
    border: 1px solid var(--border);
    border-radius: 12px;
    padding: 14px 18px;
    margin: 20px 0;
    font-size: 0.93rem;
  ">
    <h3 style="color: var(--accent); margin-bottom: 8px;">Neural Field Architecture</h3>

    <p><b>Input:</b> 2D pixel coordinate \((x, y)\)</p>

    <p><b>Positional Encoding:</b> 10 frequency bands  
       <span style="color: var(--muted);">(also compared with 2)</span>
    </p>

    <p><b>MLP:</b></p>
    <ul style="margin-left: 18px;">
      <li>Hidden width: 256 
        <span style="color: var(--muted);">(also tested 64)</span>
      </li>
      <li>Two Linear + ReLU layers</li>
      <li>Final Linear → Sigmoid (RGB)</li>
    </ul>

    <p><b>Loss:</b> MSE <b>Optimizer:</b> Adam (lr = 1e−2)</p>
  </div>

  <p>
    Below are examples of the reconstruction quality at different training iterations.
    Lower-frequency / narrower-width networks (e.g., freq=2, width=64) struggle with high-frequency
    details, while the full model (freq=10, width=256) captures the image much more accurately.
  </p>

  <!-- ─────────────────────────────────────────────── -->
  <!-- Five-image placeholder gallery -->
  <!-- ─────────────────────────────────────────────── -->
  <div class="image-grid">
    <figure>
      <img src="images/p1_original.png" alt="Original">
      <figcaption class="notes">Original image</figcaption>
    </figure>

    <figure>
      <img src="images/p1_iter50.png" alt="Iter 50">
      <figcaption class="notes">Reconstruction at 50 iterations</figcaption>
    </figure>

    <figure>
      <img src="images/p1_iter250.png" alt="Iter 250">
      <figcaption class="notes">Reconstruction at 250 iterations</figcaption>
    </figure>

    <figure>
      <img src="images/p1_iter1000.png" alt="Iter 1000">
      <figcaption class="notes">Reconstruction at 1000 iterations</figcaption>
    </figure>

    <figure>
      <img src="images/p1_final.png" alt="Final Reconstruction">
      <figcaption class="notes">Final reconstruction (3000 iterations)</figcaption>
    </figure>
  </div>

</section>

    <section id="part2" class="card">
      <h2>Part 2 — Ray Sampling</h2>
      <p>Placeholder content — stratified sampling, hierarchical sampling, etc.</p>
    </section>

    <section id="part3" class="card">
      <h2>Part 3 — NeRF Training</h2>
      <p>Placeholder content — MLP architecture, loss, training curves.</p>
    </section>

    <section id="part4" class="card">
      <h2>Part 4 — Novel View Synthesis</h2>
      <p>Placeholder content — renderings, orbit GIFs, spherical GIFs.</p>
    </section>

    <section id="results" class="card">
      <h2>Results & GIFs</h2>
      <p>Placeholder content — final GIFs and comparisons.</p>
    </section>

    <section id="conclusion" class="card">
      <h2>Conclusion</h2>
      <p>Placeholder content — reflections and analysis.</p>
    </section>

  </main>

  <!-- ===== FOOTER ===== -->
  <footer>
    <p>© 2025 Eason Wei | UC Berkeley CS180 – Neural Radiance Fields</p>
  </footer>
</body>
</html>
