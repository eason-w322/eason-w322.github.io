<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>CS180 Project 5 — Diffusion Models</title>
  <link rel="stylesheet" href="style.css"/>
  <!-- MathJax -->
<script>
  window.MathJax = {
    tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
    svg: { fontCache: 'global' }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>

<body>

<!-- ===== HEADER ===== -->
<header class="site-header">
  <h1>CS180 Project 5: Diffusion Models</h1>
  <p class="subtitle">Eason Wei | CS 180 — Fall 2025</p>
</header>

<!-- ===== SIDEBAR ===== -->

<!-- ===== MAIN CONTENT ===== -->
<main class="container">

  <section id="part0" class="card">
    <h2>Part 0: Setup</h2>

    <h3>Text Prompts Used for Prompt Embeddings</h3>

    <p>
      To explore the behavior of the DeepFloyd diffusion model, I generated prompt embeddings
      for a diverse set of text prompts ranging from highly detailed, imaginative scenes to
      very simple object-level descriptions.
    </p>

    <div class="textbox">
      <ul>
        <li>a giant library floating in the clouds, glowing with jellyfish lights</li>
        <li>a neon samurai standing in a rainy cyberpunk street</li>
        <li>a realistic cat dressed like a king in a Renaissance painting</li>
        <li>a crystal dragon coming out of a frozen waterfall at sunrise</li>
        <li>an astronaut walking inside a huge ancient temple on an alien planet</li>
        <li>a tiny house floating inside a glowing glass bubble</li>
        <li>a giant whale swimming through a sky full of clouds</li>
        <li>a robot painting a sunset on a canvas</li>
        <li>a tree growing on top of a small island floating in space</li>
        <li>a traditional Chinese ink painting of mountains, water, and a small boat</li>
        <li>a cartoon owl sitting on a branch</li>
        <li>a small lantern glowing in the night</li>
        <li>a realistic portrait of a human face</li>
        <li>a small cottage on a hill at sunset</li>
        <li>a high quality photo</li>
        <li>a rocket ship</li>
        <li>a Quinjet</li>
        <li>a pyramid</li>
        <li>an oil painting of an old man</li>
        <li>an oil painting of people around a campfire</li>
        <li>a cat</li>
        <li>a dog</li>
        <li>a landscape of a mountain range</li>
        <li>a skull</li>
        <li>a waterfall</li>
      </ul>
    </div>

    <div class="image-stack">
  <figure>
    <img src="part0_50.png" alt="Complex prompt generations">
    <figcaption>
      Results with <b>50 inference steps</b> (seed = <b>180</b>). Some semantic details are
      underdeveloped or missing due to insufficient denoising.
    </figcaption>
  </figure>
</div>

<div class="image-stack">
  <figure>
    <img src="part0_100.png" alt="Simple prompt generations">
    <figcaption>
      Results with <b>100 inference steps</b> (seed = <b>180</b>). Textures and edges are
      noticeably sharper, and previously missing semantic elements—such as the astronaut
      figure—are clearly formed.
    </figcaption>
  </figure>
</div>

  </section>

 <section id="part1_1" class="card">
  <h2>1.1 — Forward Diffusion Process</h2>

  <p>
    The forward diffusion process gradually corrupts a clean image by adding Gaussian noise.
    Given a clean image \(x_0\), the noisy image \(x_t\) at timestep \(t\) is generated by
    scaling the original signal and injecting noise according to a predefined noise schedule.
  </p>

  <p>
    Let \(\bar{\alpha}_t\) denote the cumulative noise coefficient at timestep \(t\).
    The forward process is defined as:
  </p>

  <p class="math-block">
    \[
      x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1 - \bar{\alpha}_t}\,\varepsilon,
      \quad \varepsilon \sim \mathcal{N}(0, I)
    \]
  </p>

  <p>
    Below is the implementation of the forward diffusion function. The noise coefficient
    \(\bar{\alpha}_t\) is retrieved from <code>alphas_cumprod</code>, and the noise
    \(\varepsilon\) is sampled from a standard Gaussian distribution.
  </p>

  <pre><code class="language-python">
def forward(im, t):
    alpha_bar_t = alphas_cumprod[t].to(im.device).type_as(im)
    eps = torch.randn_like(im)
    im_noisy = torch.sqrt(alpha_bar_t) * im + torch.sqrt(1 - alpha_bar_t) * eps
    return im_noisy
  </code></pre>

  <p>
    As \(t\) increases, \(\bar{\alpha}_t\) decreases, causing the original image content
    to fade while noise dominates. At large timesteps, the image approaches pure Gaussian noise.
  </p>

  <!-- ========================= -->
  <!-- Forward Diffusion Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.1.png" alt="Forward diffusion at different timesteps">
      <figcaption>
        Forward diffusion applied to the Campanile image at increasing timesteps,
        showing progressively stronger noise as \(t\) grows.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_2" class="card">
  <h2>1.2 — Classical Denoising</h2>

  <p>
    I apply
    Gaussian denoising to images corrupted at timesteps
    \(t \in \{250, 500, 750\}\).
  </p>

  <!-- ========================= -->
  <!-- Gaussian Denoising Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.2.png" alt="Gaussian denoising results">
      <figcaption>
        Gaussian-denoised Campanile images at timesteps
        \(t = 250, 500, 750\). The filtering smooths noise but primarily
        introduces blur, failing to restore meaningful structure or sharp edges.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_3" class="card">
  <h2>1.3 — One-Step Denoising</h2>

  <p>
    In one-step denoising, the pretrained diffusion model is used to estimate the
    Gaussian noise present in a noisy image \(x_t\) at timestep \(t\). Given the
    predicted noise \(\hat{\varepsilon}\) and the known noise coefficient
    \(\bar{\alpha}_t\), we can directly recover an estimate of the original clean
    image \(x_0\) in a single step.
  </p>

  <p>
    The reconstruction is performed using the following equation:
  </p>

  <pre><code>
x0_hat = (im_noisy - torch.sqrt(1 - alpha_bar) * noise_est) / torch.sqrt(alpha_bar)
  </code></pre>


  <!-- ========================= -->
  <!-- One-Step Denoising Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.3.png" alt="One-step denoising results">
      <figcaption>
        one can see that the noisier the image, the harder for 1-step denoiser to recover the clean image
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_4" class="card">
  <h2>1.4 — Iterative Denoising</h2>

  <!-- ========================= -->
  <!-- Iterative Denoising Loop -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.4_1.png" alt="Iterative denoising progression">
      <figcaption>
        Iterative denoising progression starting from timestep
        <code>strided_timesteps[10]</code>. The image is shown every 5 denoising steps.
        Noise is gradually removed and semantic structure emerges as the timestep
        decreases.
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Method Comparison -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.4_2.png" alt="Denoising method comparison">
      <figcaption>
        Comparison of denoising methods using the same starting noisy image
        (<code>i_start = 10</code>). From left to right: iterative diffusion denoising,
        one-step denoising, and Gaussian blurring. Iterative denoising produces the
        sharpest result and best preserves global structure, while one-step denoising
        degrades at higher noise levels and Gaussian blurring fails to recover meaningful
        details.
      </figcaption>
    </figure>
  </div>
</section>


  <section id="part1_5" class="card">
  <h2>1.5 — Diffusion Model Sampling</h2>
  <!-- ========================= -->
  <!-- Sampling Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.5.png" alt="Diffusion sampling results">
      <figcaption>
        Five samples generated from pure Gaussian noise using iterative diffusion
        sampling with the prompt <code>"a high quality photo"</code>. Each image
        is produced from a different random noise initialization.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_6" class="card">
  <h2>1.6 — Classifier-Free Guidance (CFG)</h2>

  <p>
    The final guided noise estimate is computed using the following equation:
  </p>

  <pre><code>
noise_est = noise_u + scale * (noise_c - noise_u)
  </code></pre>

  <p>
    Here, <code>scale</code> controls the strength of guidance. In my implementation,
    I use the prompt <code>"a high quality photo"</code> as the conditional input,
    and an empty string <code>""</code> as the unconditional input. Larger guidance
    scales improve prompt adherence at the cost of reduced sample diversity.
  </p>

  <!-- ========================= -->
  <!-- CFG Sampling Results -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.6.png" alt="CFG sampling results">
      <figcaption>
        Five images generated using classifier-free guidance with
        <code>scale = 7</code>. Compared to unguided sampling, CFG produces sharper
        images with stronger semantic alignment to the prompt
        <code>"a high quality photo"</code>.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_7" class="card">
  <h2>1.7 — Image-to-Image Translation</h2>

  <!-- ========================= -->
  <!-- Campanile Edits -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.png" alt="Campanile image-to-image edits">
      <figcaption>
        Image-to-image translation results for the Campanile image using CFG.
        Edits are generated with starting indices
        <code>[1, 3, 5, 7, 10, 20]</code>  with conditional text prompt "a high quality photo", showing progressively stronger deviations
        from the original image as the noise level increases.
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Test Image 1 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7_1.png" alt="Test image 1 edits">
      <figcaption>

      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Test Image 2 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7_2.png" alt="Test image 2 edits">
      <figcaption>
   
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_7_1" class="card">
  <h2>1.7.1 — Editing Hand-Drawn and Web Images</h2>
  <!-- ========================= -->
  <!-- Web Image Editing -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.1.1.png" alt="Web image edits">
      <figcaption>
        Image-to-image translation applied to a butterfly image downloaded
        from the web. Results are shown for starting indices
        <code>[1, 3, 5, 7, 10, 20]</code>, demonstrating increasing deviation from the
        original image as noise increases.
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Hand-Drawn Image 1 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.1.2.png" alt="Hand-drawn image 1 edits">
      <figcaption>
        Image-to-image translation applied to a hand-drawn emoji image
      </figcaption>
    </figure>
  </div>

  <!-- ========================= -->
  <!-- Hand-Drawn Image 2 -->
  <!-- ========================= -->
  <div class="image-stack">
    <figure>
      <img src="1.7.1.3.png" alt="Hand-drawn image 2 edits">
      <figcaption>
        Image-to-image translation applied to a second hand-drawn tree image.
      </figcaption>
    </figure>
  </div>
</section>

  <section id="part1_7_2" class="card">
  <h2>1.7.2 — Inpainting</h2>

  <pre><code>
noised_original = forward(original_image, prev_t).half().to(device)
pred_prev_image = mask * pred_prev_image + (1 - mask) * noised_original
  </code></pre>

  <p>
    This ensures that only pixels inside the mask are modified by the diffusion process,
    while all other regions remain faithful to the original image.
  </p>

  <!-- ===== Image Stack 1 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.2.1.png" alt="Original Campanile">
      <figcaption>
        squre mask at the top of campanile
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 2 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.2.2.png" alt="Inpainting mask">
      <figcaption>
        pumpkins with a circular mask
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 3 ===== -->
  <div class="image-stack">
    <figure>
      <img src="1.7.2.3.png" alt="Inpainted Campanile">
      <figcaption>
        sky with bird masked out
      </figcaption>
    </figure>
  </div>

</section>

  <section id="part1_7_3" class="card">
  <h2>1.7.3 — Text-Conditional Image-to-Image Translation</h2>

  <p>
    In this section, we extend the SDEdit-style image-to-image translation by introducing
    <b>text conditioning</b>. Instead of projecting the noisy image back onto the natural
    image manifold alone, we guide the denoising process using a text prompt, allowing us
    to control the semantic direction of the edit.
  </p>

  <p>
    This is implemented by simply replacing the default prompt
    <code>"a high quality photo"</code> with a custom text prompt, while continuing to use
    classifier-free guidance (CFG). As the noise level increases, the generated images
    gradually move away from the original image and increasingly reflect the text prompt.
  </p>

  <!-- ===== Image Stack 1 ===== -->
  <div class="image-stack">
    <figure>
      <img src="rocket_noise_levels.png" alt="Rocket ship edits at different noise levels">
      <figcaption>
        Campanile image edited using the text prompt <b>"a rocket ship"</b> at increasing
        noise levels. Higher noise allows the text prompt to dominate the structure of the
        image.
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 2 ===== -->
  <div class="image-stack">
    <figure>
      <img src="custom_prompt_image1.png" alt="Custom image edit 1">
      <figcaption>
        Text-guided edits applied to a second test image using a custom prompt, showing a
        smooth transition from the original image to the prompted concept.
      </figcaption>
    </figure>
  </div>

  <!-- ===== Image Stack 3 ===== -->
  <div class="image-stack">
    <figure>
      <img src="custom_prompt_image2.png" alt="Custom image edit 2">
      <figcaption>
        Another example of text-conditional image-to-image translation, demonstrating how
        the diffusion model balances visual similarity to the original image with semantic
        alignment to the prompt.
      </figcaption>
    </figure>
  </div>

</section>

</main>

<!-- ===== FOOTER ===== -->
<footer>
  <p>© 2025 Eason Wei | UC Berkeley CS180</p>
</footer>

</body>
</html>
